{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10773it [2:38:19,  1.13it/s]\n",
      "100%|██████████| 1348/1348 [20:12<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25: train loss:  0.4912, val loss: 0.5121\n",
      "step 25: train accuracy:  80.44%, val accuracy: 79.41%\n",
      "Iter [1/100] - Elapsed Time: 9499.22s  Remaining Time: [949921.94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:39:00,  1.13it/s]\n",
      "100%|██████████| 1348/1348 [20:07<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 26: train loss:  0.4985, val loss: 0.5127\n",
      "step 26: train accuracy:  80.75%, val accuracy: 79.54%\n",
      "Iter [2/100] - Elapsed Time: 20252.19s  Remaining Time: [1002483.48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:39:32,  1.13it/s]\n",
      "100%|██████████| 1348/1348 [20:33<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 27: train loss:  0.5010, val loss: 0.5114\n",
      "step 27: train accuracy:  80.57%, val accuracy: 79.43%\n",
      "Iter [3/100] - Elapsed Time: 31032.45s  Remaining Time: [1013726.76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:40:44,  1.12it/s]\n",
      "100%|██████████| 1348/1348 [20:27<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 28: train loss:  0.4935, val loss: 0.5134\n",
      "step 28: train accuracy:  80.84%, val accuracy: 79.23%\n",
      "Iter [4/100] - Elapsed Time: 41910.95s  Remaining Time: [1016340.63]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:42:23,  1.11it/s]\n",
      "100%|██████████| 1348/1348 [20:24<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 29: train loss:  0.5017, val loss: 0.5127\n",
      "step 29: train accuracy:  81.20%, val accuracy: 79.27%\n",
      "Iter [5/100] - Elapsed Time: 52881.42s  Remaining Time: [1015323.27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:40:34,  1.12it/s]\n",
      "100%|██████████| 1348/1348 [20:32<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30: train loss:  0.4979, val loss: 0.5121\n",
      "step 30: train accuracy:  81.01%, val accuracy: 79.45%\n",
      "Iter [6/100] - Elapsed Time: 63741.11s  Remaining Time: [1009234.27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:41:44,  1.11it/s]\n",
      "100%|██████████| 1348/1348 [20:33<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31: train loss:  0.4910, val loss: 0.5114\n",
      "step 31: train accuracy:  80.91%, val accuracy: 79.28%\n",
      "Iter [7/100] - Elapsed Time: 74678.17s  Remaining Time: [1002821.09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "8944it [2:14:22,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interrupt,\n",
      "Final train loss: 0.4910, \n",
      "Final val loss: 0.5114, \n",
      "Final train accuracy: 80.91%, \n",
      "Final val accuracy: 79.28%\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "'''using torchvision for dataloading'''\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D,MobileNetLarge3D, MobileNetTiny3D \n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy, average_for_plotting\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#hyperparameters\n",
    "torch.manual_seed(1234)\n",
    "learning_rate = 1e-6 #the paper quotes rmsprop with 0.1 lr, but we have a tiny batch size, and are using AdamW\n",
    "batch_size = 4 #the paper quotes 128 images/chip, but with video we have to change this\n",
    "max_iters = 100\n",
    "eval_interval = 1 \n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "eps = np.sqrt(0.002) #From the pytorch blog post, \"a reasonable approximation can be taken with the formula PyTorch_eps = sqrt(TF_eps).\"\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "use_autocast = False \n",
    "compile = False\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/workspace/picklebotdataset/train'\n",
    "val_video_paths = '/workspace/picklebotdataset/val'\n",
    "\n",
    "#annotations paths\n",
    "train_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/train_labels.csv'\n",
    "val_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/val_labels.csv'\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/train_all_together'\n",
    "val_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/val_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "#dataset     \n",
    "train_dataset = PicklebotDataset(train_annotations_file,train_video_paths,transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "val_dataset = PicklebotDataset(val_annotations_file,val_video_paths,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "#define model, initialize weights \n",
    "model = MobileNetSmall3D()\n",
    "model.initialize_weights()\n",
    "model = model.to(device)\n",
    "\n",
    "#for multi-gpu\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# optimizer\n",
    "# optimizer = optim.RMSprop(params=model.parameters(),lr=learning_rate,weight_decay=weight_decay,momentum=momentum,eps=eps) #starting with AdamW for now. \n",
    "optimizer = optim.AdamW(params=model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#cosine annealing\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "#loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_autocast:\n",
    "    scaler = GradScaler()\n",
    "model_name = 'mobilenet_3d'\n",
    "writer = SummaryWriter(f'runs/{model_name}') #tensorboard writer \n",
    "model.load_state_dict(torch.load(f'old_checkpoints/mobilenet_3d36.pth')) #if applicable, load the model from the last checkpoint\n",
    "\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2 and a modern gpu, these lines are from karpathy\n",
    "    print(\"compilation complete!\")\n",
    "\n",
    "\n",
    "#estimate loss using the val set, and calculate accuracy\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    val_losses = [] \n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for val_features,val_labels in tqdm(val_loader):\n",
    "        val_features = val_features.to(device)\n",
    "        val_labels = val_labels.to(torch.int64) #waiting to move to device until after forward pass, idk if this matters\n",
    "        # val_labels = val_labels.expand(val_features.shape[2]) #this is only for our lstm T -> batch size, a lame hack    \n",
    "        val_outputs = model(val_features)\n",
    "\n",
    "        val_loss = criterion(val_outputs,val_labels.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        val_correct += calculate_accuracy(val_outputs,val_labels)\n",
    "        val_samples += len(val_labels)\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "#try except block so we can manually early stop while saving the model\n",
    "#training loop\n",
    "start_time = time.time()\n",
    "train_losses = torch.tensor([])\n",
    "train_percent = torch.tensor([])\n",
    "val_losses = []\n",
    "val_percent = []\n",
    "counter = 0\n",
    "\n",
    "try:\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        batch_loss_list = []\n",
    "        batch_percent_list = []\n",
    "\n",
    "        #forward pass\n",
    "        for batch_idx, (features,labels) in tqdm(enumerate(train_loader)):\n",
    "            labels = labels.to(torch.int64)\n",
    "            features = features.to(device)\n",
    "            # labels = labels.expand(features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "            \n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if use_autocast:    \n",
    "                with autocast():\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs,labels.to(device))\n",
    "                \n",
    "                #backprop & update weights\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs,labels.to(device))\n",
    "\n",
    "                #backprop & update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            \n",
    "\n",
    "            batch_loss_list.append(loss.item()) #append the loss of the batch to our list to be averaged and plotted later, this is dataset size / batch size long\n",
    "            batch_correct = calculate_accuracy(outputs,labels) #number of correct predictions in the batch\n",
    "            train_correct += batch_correct #this is the total number of correct predictions so far\n",
    "            train_samples += len(labels) #this is the total number of samples so far\n",
    "            batch_percent_list.append(train_correct/train_samples)\n",
    "            writer.add_scalar('training loss', batch_loss_list[-1], counter)\n",
    "            writer.add_scalar('training accuracy', batch_percent_list[-1], counter)\n",
    "            counter += 1\n",
    "\n",
    "        train_losses = torch.cat((train_losses,average_for_plotting(batch_loss_list))) #train losses is a tensor\n",
    "        train_percent = torch.cat((train_percent,average_for_plotting(batch_percent_list))) #train percent is a tensor\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_iters = max_iters - iter\n",
    "        avg_time_per_iter = elapsed / (iter + 1)\n",
    "        estimated_remaining_time = remaining_iters * avg_time_per_iter\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                        \n",
    "            #evaluate the model\n",
    "            val_loss, val_accuracy = estimate_loss()\n",
    "        \n",
    "            val_losses.append(val_loss) #average loss of the val dataset, this is a scalar\n",
    "            val_percent.append(val_accuracy) #percent of correct predictions in the val set, this is a scalar\n",
    "\n",
    "\n",
    "            print(f\"step {iter+25}: train loss:  {train_losses[-1].mean().item():.4f}, val loss: {val_losses[-1]:.4f}\") #report the average loss of the batch\n",
    "            print(f\"step {iter+25}: train accuracy:  {(train_percent[-1].mean().item())*100:.2f}%, val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "            writer.add_scalar('val loss', val_losses[-1], iter+37)\n",
    "            writer.add_scalar('val accuracy',val_percent[-1], iter+37)\n",
    "            torch.save(model.state_dict(), f'checkpoints/{model_name}{iter+37}.pth')\n",
    "\n",
    "        tqdm.write(f\"Iter [{iter+1}/{max_iters}] - Elapsed Time: {elapsed:.2f}s  Remaining Time: [{estimated_remaining_time:.2f}]\")\n",
    "        if iter == max_iters -1:\n",
    "            print(\"Training completed:\") \n",
    "            print(f\"Final train loss: {train_losses[-1].mean().item():.4f},\")\n",
    "            print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "            print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "            print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Keyboard interrupt,\\nFinal train loss: {train_losses[-1].mean().item():.4f}, \")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "    print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "    print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), f'checkpoints/{model_name}_finished.pth')\n",
    "    with open(f'statistics/{model_name}_finished_train_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_losses))\n",
    "    with open(f'statistics/{model_name}_finished_val_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_losses))\n",
    "    with open(f'statistics/{model_name}_finished_train_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_percent))\n",
    "    with open(f'statistics/{model_name}_finished_val_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_percent))\n",
    "    print(f\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This version of the program uses Nvidia Dali to load data, not torchvision.io.read_video,\n",
    "   It should be substantially faster, especially with multiple gpus, perhaps a good setup \n",
    "   would be one to load the videos, one to run the training loop? Perhaps not as I learned more about it.\n",
    "\n",
    "    Eventually, this and the other version in this notebook should be merged into one notebook, with a flag to choose which to use.\n",
    "   \n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchvision.io import write_video\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D, MobileNetLarge3D, MobileNetTiny3D\n",
    "from helpers import calculate_accuracy, video_pipeline, average_for_plotting\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "'''\n",
    "Our mean is ([0.3939, 0.3817, 0.3314])\n",
    "Our std is ([0.2104, 0.1986, 0.1829])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''Strikes are 0, balls 1.'''\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#hyperparameters\n",
    "learning_rate = 3e-4 #the paper quotes rmsprop with 0.1 lr, but we have a tiny batch size, and are using AdamW\n",
    "batch_size = 4 #the paper quotes 128 images/chip, but with video we have to change this\n",
    "max_iters = 300\n",
    "eval_interval = 2\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "eps = np.sqrt(0.002) #From the pytorch blog post, \"a reasonable approximation can be taken with the formula PyTorch_eps = sqrt(TF_eps).\"\n",
    "std = torch.tensor([0.2104, 0.1986, 0.1829])[None,None,None,:]\n",
    "mean = torch.tensor([0.3539, 0.3817, 0.3314])[None,None,None,:]\n",
    "use_autocast = True\n",
    "compile = False\n",
    "\n",
    "#information for the dali pipeline\n",
    "sequence_length = 199 #longest videos in our dataset (changed to smaller number for testing)\n",
    "initial_prefetch_size = 20\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/workspace/picklebotdataset/train'\n",
    "val_video_paths = '/workspace/picklebotdataset/val'\n",
    "\n",
    "train_video_paths = '/home/hankhome/Documents/PythonProjects/picklebotdataset/train'\n",
    "val_video_paths = '/home/hankhome/Documents/PythonProjects/picklebotdataset/val'\n",
    "num_train_videos = len(os.listdir(train_video_paths + '/' + 'balls')) + len(os.listdir(train_video_paths + '/' + 'strikes'))\n",
    "num_val_videos = len(os.listdir(val_video_paths + '/' + 'balls')) + len(os.listdir(val_video_paths + '/' + 'strikes'))\n",
    "\n",
    "#define our model, initialize weights\n",
    "model = MobileNetSmall3D()\n",
    "# model.initialize_weights()\n",
    "model = model.to(device)\n",
    "\n",
    "#for multi-gpu setups \n",
    "#may want to revisit this and choose which device we use for loading with dali, and which to use for training the net.\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "#define our optimizer\n",
    "#optimizer = optim.RMSprop(params=model.parameters(),lr=learning_rate,weight_decay=weight_decay,momentum=momentum,eps=eps) #starting with AdamW for now. \n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "scaler = GradScaler()\n",
    "model_name = 'mobilenetsmall_3D_cloud' \n",
    "# model.load_state_dict(torch.load(f'{model_name}.pth')) #if applicable, load the model from the last checkpoint\n",
    "\n",
    "\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2 and a modern gpu, these lines were lifted from karpathy\n",
    "    print(\"compilation complete!\")\n",
    "\n",
    "#estimate_loss using validation set, we should refactor this.\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for _,val_features in tqdm(enumerate(val_loader)):\n",
    "        val_labels = (val_features[0]['label']).view(-1).long() #need this as a (batch_size,) tensor\n",
    "        val_features = val_features[0]['data']/255\n",
    "        # val_features = (val_features-mean.to(device))/std.to(device) #normalize\n",
    "        val_features = val_features.permute(0,-1,1,2,3) \n",
    "        # val_labels = val_labels.expand(val_features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "\n",
    "        val_outputs = model(val_features)\n",
    "        \n",
    "        val_loss = criterion(val_outputs,val_labels)\n",
    "        \n",
    "        val_losses.append(val_loss.item())  \n",
    "        \n",
    "        val_correct += calculate_accuracy(val_outputs,val_labels) #get number of correct\n",
    "        val_samples += len(labels) #this is the total number of samples so far\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "#initialize lists for plotting\n",
    "start_time = time.time()\n",
    "train_losses = torch.tensor([])\n",
    "train_percent = torch.tensor([])\n",
    "val_losses = []\n",
    "val_percent = []\n",
    "\n",
    "#plot losses\n",
    "plt.ion()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,4))\n",
    "fig.show()\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Val Loss')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "#plot accuracy\n",
    "ax2.plot(train_percent, label='Train Accuracy')\n",
    "ax2.plot(val_percent, label='Val Accuracy')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "#build our pipelines\n",
    "train_pipe = video_pipeline(batch_size=batch_size, num_threads=cpu_count(), device_id=0, file_root=train_video_paths,\n",
    "                            sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "val_pipe = video_pipeline(batch_size=batch_size, num_threads=cpu_count(), device_id=0, file_root=val_video_paths,\n",
    "                          sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "\n",
    "train_pipe.build()\n",
    "val_pipe.build()\n",
    "\n",
    "\n",
    "train_loader = DALIClassificationIterator(train_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_train_videos)\n",
    "val_loader = DALIClassificationIterator(train_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_train_videos)\n",
    "\n",
    "try:\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        batch_loss_list = [] #want to overwrite this each epoch\n",
    "        batch_percent_list = []\n",
    "\n",
    "        #forward pass\n",
    "        for batch_idx, features in tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            labels = (features[0]['label']).view(-1).long() #need this as a (batch_size,) tensor in int64\n",
    "            features = features[0]['data']/255 #i think it makes sense to overwrite features to save precious gpu memory\n",
    "            # features = (features-mean.to(device))/std.to(device) #normalize\n",
    "            features = features.permute(0,-1,1,2,3) #reshape for our 3D convolutions\n",
    "            # labels = labels.expand(features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "            \n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if use_autocast:    \n",
    "                with autocast():\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs,labels)\n",
    "                \n",
    "                #backprop & update weights\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs,labels)\n",
    "\n",
    "                #backprop & update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_loss_list.append(loss.item()) #append the loss of the batch to our list to be averaged and plotted later, this is dataset size / batch size long\n",
    "            batch_correct = calculate_accuracy(outputs,labels) #number of correct predictions in the batch\n",
    "            train_correct += batch_correct #this is the total number of correct predictions so far\n",
    "            train_samples += len(labels) #this is the total number of samples so far\n",
    "            batch_percent_list.append(train_correct/train_samples)\n",
    "\n",
    "        train_losses = torch.cat((train_losses,average_for_plotting(batch_loss_list))) #train losses is a tensor\n",
    "        train_percent = torch.cat((train_percent,average_for_plotting(batch_percent_list))) #train percent is a tensor\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_iters = max_iters - iter\n",
    "        avg_time_per_iter = elapsed / (iter + 1)\n",
    "        estimated_remaining_time = remaining_iters * avg_time_per_iter\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                        \n",
    "            #evaluate the model\n",
    "            val_loss, val_accuracy = estimate_loss()\n",
    "            print(len(batch_loss_list))\n",
    "            val_losses.append(val_loss) #average loss of the val dataset, this is a scalar\n",
    "            val_percent.append(val_accuracy) #percent of correct predictions in the val set, this is a scalar\n",
    "\n",
    "\n",
    "            print(f\"step {iter}: train loss:  {train_losses[-1].mean().item():.4f}, val loss: {val_losses[-1]:.4f}\") #report the average loss of the batch\n",
    "            print(f\"step {iter}: train accuracy:  {(train_percent[-1].mean().item())*100:.2f}%, val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "            # #plot the losses\n",
    "            ax1.plot(train_losses, label='Train Loss')\n",
    "            ax1.plot(val_losses, label='Val Loss')\n",
    "\n",
    "            #plot the accuracy\n",
    "            ax2.plot(train_percent, label='Train Accuracy')\n",
    "            ax2.plot(val_percent, label='Val Accuracy')\n",
    "\n",
    "\n",
    "            fig.canvas.draw()\n",
    "            fig.canvas.flush_events()\n",
    "            plt.pause(1)\n",
    "            time.sleep(1)\n",
    "\n",
    "        tqdm.write(f\"Iter [{iter+1}/{max_iters}] - Elapsed Time: {elapsed:.2f}s  Remaining Time: [{estimated_remaining_time:.2f}]\")\n",
    "        if iter == max_iters -1:\n",
    "            print(\"Training completed:\") \n",
    "            print(f\"Final train loss: {train_losses[-1].mean().item():.4f},\")\n",
    "            print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "            print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "            print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\") \n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Keyboard interrupt,\\nFinal train loss: {train_losses[-1].mean().item():.4f}, \")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "    print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "    print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), f'{model_name}.pth')\n",
    "    with open(f'{model_name}_train_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_losses))\n",
    "    with open(f'{model_name}_val_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_losses))\n",
    "    with open(f'{model_name}_train_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_percent))\n",
    "    with open(f'{model_name}_val_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_percent))\n",
    "    print(f\"Model saved!\") \n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = np.load('old_statistics/mobilenet_3d_val_percent.npy')\n",
    "second_run = np.load('old_statistics/tuned_run_statistics/mobilenet_3d_finished_val_percent.npy')\n",
    "third_run = np.load('statistics/mobilenet_3d_finished_val_percent.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7806008902077152, 24)\n",
      ",(0.7967359050445104, 10)\n",
      ",(0.7954376854599406, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{max(first_run),np.argmax(first_run)}\\n,{max(second_run),np.argmax(second_run)}\\n,{max(third_run),np.argmax(third_run)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5249487713907875, 24)\n",
      ",(0.5114866618498143, 11)\n",
      ",(0.5114228287905189, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_run = np.load('old_statistics/mobilenet_3d_val_losses.npy')\n",
    "second_run = np.load('old_statistics/tuned_run_statistics/mobilenet_3d_finished_val_losses.npy')\n",
    "third_run = np.load('statistics/mobilenet_3d_finished_val_losses.npy')\n",
    "\n",
    "print(f'{min(first_run),np.argmin(first_run)}\\n,{min(second_run),np.argmin(second_run)}\\n,{min(third_run),np.argmin(third_run)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32,), (15,), 47)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_run.shape, second_run.shape, 32+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the ones to try:\\ncheckpoint 24 from the first run,\\ncheckpoints 10 and 11 from the second run,\\ncheckpoints 1 and 2 from the third run.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''the ones to try:\n",
    "checkpoint 24 from the first run,\n",
    "checkpoints 10 and 11 from the second run,\n",
    "checkpoints 1 and 2 from the third run.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_48450.mp4 276\n",
      "clip_20148.mp4 245\n",
      "clip_17269.mp4 276\n",
      "clip_7047.mp4 276\n",
      "clip_27283.mp4 276\n",
      "clip_24594.mp4 276\n",
      "clip_27224.mp4 276\n",
      "clip_24352.mp4 276\n",
      "clip_18329.mp4 276\n",
      "clip_44836.mp4 276\n",
      "clip_38430.mp4 368\n",
      "clip_47638.mp4 276\n",
      "clip_49158.mp4 270\n",
      "clip_46556.mp4 276\n",
      "clip_48180.mp4 276\n",
      "clip_8489.mp4 276\n",
      "clip_39407.mp4 276\n",
      "clip_33335.mp4 276\n",
      "clip_34081.mp4 276\n",
      "clip_49346.mp4 270\n",
      "clip_34560.mp4 200\n",
      "clip_21747.mp4 276\n",
      "clip_7065.mp4 276\n",
      "clip_10676.mp4 276\n",
      "clip_27954.mp4 276\n",
      "clip_11492.mp4 276\n",
      "clip_11927.mp4 390\n",
      "clip_34675.mp4 276\n",
      "clip_25504.mp4 276\n",
      "clip_36622.mp4 276\n",
      "clip_4006.mp4 276\n",
      "clip_16420.mp4 368\n",
      "clip_31911.mp4 276\n",
      "clip_51956.mp4 270\n",
      "clip_24799.mp4 276\n",
      "clip_49336.mp4 270\n",
      "clip_43768.mp4 205\n",
      "clip_45951.mp4 276\n",
      "clip_24119.mp4 276\n",
      "clip_29228.mp4 276\n",
      "clip_20269.mp4 276\n",
      "clip_14982.mp4 276\n",
      "clip_25632.mp4 276\n",
      "clip_42489.mp4 276\n",
      "clip_2949.mp4 276\n",
      "clip_14187.mp4 276\n",
      "clip_46510.mp4 276\n",
      "clip_33532.mp4 276\n",
      "clip_7620.mp4 276\n",
      "clip_48452.mp4 276\n",
      "clip_124.mp4 240\n",
      "clip_8815.mp4 312\n",
      "clip_30812.mp4 276\n",
      "clip_16216.mp4 276\n",
      "clip_10144.mp4 276\n",
      "clip_11968.mp4 551\n",
      "clip_37858.mp4 276\n",
      "clip_960.mp4 276\n",
      "clip_49355.mp4 360\n",
      "clip_5314.mp4 276\n",
      "clip_38590.mp4 276\n",
      "clip_14204.mp4 276\n",
      "clip_29397.mp4 276\n",
      "clip_33259.mp4 276\n",
      "clip_45294.mp4 276\n",
      "clip_42678.mp4 305\n",
      "clip_29407.mp4 276\n",
      "clip_30356.mp4 276\n",
      "clip_25809.mp4 276\n",
      "clip_45185.mp4 222\n",
      "clip_22672.mp4 276\n",
      "clip_1968.mp4 276\n",
      "clip_20856.mp4 276\n",
      "clip_35921.mp4 276\n",
      "clip_34082.mp4 276\n",
      "clip_43355.mp4 276\n",
      "clip_22473.mp4 276\n",
      "clip_11929.mp4 276\n",
      "clip_25118.mp4 276\n",
      "clip_31968.mp4 276\n",
      "clip_27649.mp4 276\n",
      "clip_8156.mp4 276\n",
      "clip_29760.mp4 276\n",
      "clip_51903.mp4 270\n",
      "clip_34626.mp4 276\n",
      "clip_9177.mp4 276\n",
      "clip_19336.mp4 276\n",
      "clip_36109.mp4 276\n",
      "clip_720.mp4 276\n",
      "clip_26828.mp4 276\n",
      "clip_35023.mp4 276\n",
      "clip_39778.mp4 276\n",
      "clip_44239.mp4 276\n",
      "clip_6555.mp4 276\n",
      "clip_18154.mp4 276\n",
      "clip_7754.mp4 276\n",
      "clip_34571.mp4 276\n",
      "clip_28712.mp4 276\n",
      "clip_3521.mp4 276\n",
      "clip_43085.mp4 276\n",
      "clip_42916.mp4 276\n",
      "clip_1608.mp4 276\n",
      "clip_51950.mp4 360\n",
      "clip_36190.mp4 276\n",
      "clip_26725.mp4 207\n",
      "clip_27872.mp4 276\n",
      "clip_32008.mp4 241\n",
      "clip_49176.mp4 270\n",
      "clip_46988.mp4 276\n",
      "clip_40743.mp4 201\n",
      "clip_41243.mp4 276\n",
      "clip_33381.mp4 276\n",
      "clip_39657.mp4 276\n",
      "clip_32213.mp4 276\n",
      "clip_42501.mp4 276\n",
      "clip_32577.mp4 368\n",
      "clip_39159.mp4 276\n",
      "clip_38876.mp4 276\n",
      "clip_24959.mp4 223\n",
      "clip_38356.mp4 276\n",
      "clip_38022.mp4 276\n",
      "clip_45269.mp4 257\n",
      "clip_38533.mp4 276\n",
      "clip_49798.mp4 301\n",
      "clip_6203.mp4 276\n",
      "clip_12718.mp4 276\n",
      "clip_42237.mp4 276\n",
      "clip_41397.mp4 276\n",
      "clip_42483.mp4 276\n",
      "clip_34281.mp4 368\n",
      "clip_19213.mp4 276\n",
      "clip_17011.mp4 276\n",
      "clip_46700.mp4 208\n",
      "clip_9171.mp4 276\n",
      "clip_31218.mp4 276\n",
      "clip_49512.mp4 270\n",
      "clip_36687.mp4 276\n",
      "clip_48232.mp4 276\n",
      "clip_41059.mp4 234\n",
      "clip_18017.mp4 276\n",
      "clip_49101.mp4 270\n",
      "clip_11366.mp4 276\n",
      "clip_24915.mp4 204\n",
      "clip_41993.mp4 276\n",
      "clip_24261.mp4 276\n",
      "clip_36013.mp4 368\n",
      "clip_11438.mp4 276\n",
      "clip_37806.mp4 276\n",
      "clip_13570.mp4 276\n",
      "clip_49253.mp4 270\n",
      "clip_39144.mp4 276\n",
      "clip_38077.mp4 233\n",
      "clip_19619.mp4 276\n",
      "clip_34471.mp4 276\n",
      "clip_7643.mp4 276\n",
      "clip_51925.mp4 270\n",
      "clip_7618.mp4 276\n",
      "clip_290.mp4 206\n",
      "clip_36241.mp4 276\n",
      "clip_33288.mp4 276\n",
      "clip_21663.mp4 276\n",
      "clip_3382.mp4 276\n",
      "clip_10945.mp4 207\n",
      "clip_29239.mp4 368\n",
      "clip_49142.mp4 270\n",
      "clip_4253.mp4 276\n",
      "clip_944.mp4 276\n",
      "clip_26057.mp4 276\n",
      "clip_38927.mp4 276\n",
      "clip_25230.mp4 276\n",
      "clip_52041.mp4 270\n",
      "clip_6854.mp4 276\n",
      "clip_10142.mp4 276\n",
      "clip_19118.mp4 276\n",
      "clip_22028.mp4 276\n",
      "clip_5693.mp4 276\n",
      "clip_40272.mp4 276\n",
      "clip_3280.mp4 276\n",
      "clip_37769.mp4 276\n",
      "clip_23817.mp4 276\n",
      "clip_38498.mp4 276\n",
      "clip_22213.mp4 276\n",
      "clip_13070.mp4 276\n",
      "clip_27586.mp4 276\n",
      "clip_31181.mp4 276\n",
      "clip_47888.mp4 276\n",
      "clip_38423.mp4 213\n",
      "clip_46341.mp4 276\n",
      "clip_38324.mp4 276\n",
      "clip_18672.mp4 251\n",
      "clip_23836.mp4 276\n",
      "clip_19951.mp4 276\n",
      "clip_34809.mp4 276\n",
      "clip_38293.mp4 276\n",
      "clip_44080.mp4 276\n",
      "clip_12446.mp4 276\n",
      "clip_41842.mp4 276\n",
      "clip_47464.mp4 266\n",
      "clip_42372.mp4 276\n",
      "clip_44376.mp4 276\n",
      "clip_24212.mp4 276\n",
      "clip_35158.mp4 276\n",
      "clip_21014.mp4 276\n",
      "clip_30096.mp4 276\n",
      "clip_1103.mp4 276\n",
      "clip_48800.mp4 270\n",
      "clip_12757.mp4 276\n",
      "clip_45090.mp4 276\n",
      "clip_35493.mp4 276\n",
      "clip_9611.mp4 276\n",
      "clip_47860.mp4 276\n",
      "clip_28917.mp4 276\n",
      "clip_38467.mp4 276\n",
      "clip_7722.mp4 276\n",
      "clip_11273.mp4 276\n",
      "clip_48798.mp4 209\n",
      "clip_37129.mp4 276\n",
      "clip_34085.mp4 276\n",
      "clip_16432.mp4 276\n",
      "clip_42188.mp4 276\n",
      "clip_49448.mp4 270\n",
      "clip_46592.mp4 276\n",
      "clip_25405.mp4 213\n",
      "clip_4853.mp4 201\n",
      "clip_31745.mp4 276\n",
      "clip_28109.mp4 276\n",
      "clip_36084.mp4 201\n",
      "clip_45064.mp4 276\n",
      "clip_31619.mp4 234\n",
      "clip_21433.mp4 206\n",
      "clip_18821.mp4 276\n",
      "clip_14641.mp4 206\n",
      "clip_18246.mp4 229\n",
      "clip_31557.mp4 276\n",
      "clip_8203.mp4 276\n",
      "clip_40630.mp4 204\n",
      "clip_28876.mp4 276\n",
      "clip_11460.mp4 212\n",
      "clip_6096.mp4 276\n",
      "clip_19001.mp4 276\n",
      "clip_36508.mp4 276\n",
      "clip_40013.mp4 276\n",
      "clip_46701.mp4 276\n",
      "clip_31145.mp4 368\n",
      "clip_34900.mp4 276\n",
      "clip_47605.mp4 276\n",
      "clip_16257.mp4 276\n",
      "clip_47232.mp4 276\n",
      "clip_49210.mp4 270\n",
      "clip_32543.mp4 276\n",
      "clip_5492.mp4 276\n",
      "clip_43853.mp4 232\n",
      "clip_18086.mp4 203\n",
      "clip_38124.mp4 276\n",
      "clip_51947.mp4 270\n",
      "clip_44214.mp4 276\n",
      "clip_29089.mp4 276\n",
      "clip_36685.mp4 205\n",
      "clip_8875.mp4 276\n",
      "clip_20824.mp4 276\n",
      "clip_43661.mp4 276\n",
      "clip_35976.mp4 276\n",
      "clip_38602.mp4 276\n",
      "clip_42986.mp4 276\n",
      "clip_23609.mp4 231\n",
      "clip_28128.mp4 276\n",
      "clip_30361.mp4 276\n",
      "clip_23499.mp4 276\n",
      "clip_30895.mp4 276\n",
      "clip_38745.mp4 276\n",
      "clip_37989.mp4 276\n",
      "clip_41635.mp4 276\n",
      "clip_37296.mp4 276\n",
      "clip_34357.mp4 276\n",
      "clip_3102.mp4 276\n",
      "clip_38688.mp4 276\n",
      "clip_21892.mp4 276\n",
      "clip_3260.mp4 276\n",
      "clip_27319.mp4 276\n",
      "clip_41424.mp4 276\n",
      "clip_18859.mp4 276\n",
      "clip_24095.mp4 214\n",
      "clip_9236.mp4 276\n",
      "clip_33863.mp4 276\n",
      "clip_22988.mp4 368\n",
      "clip_42937.mp4 276\n",
      "clip_19348.mp4 276\n",
      "clip_24689.mp4 276\n",
      "clip_47941.mp4 276\n",
      "clip_23717.mp4 276\n",
      "clip_42554.mp4 276\n",
      "clip_31999.mp4 276\n",
      "clip_13939.mp4 276\n",
      "clip_6500.mp4 276\n",
      "clip_25591.mp4 217\n",
      "clip_945.mp4 276\n",
      "clip_8680.mp4 220\n",
      "clip_27329.mp4 235\n",
      "clip_49226.mp4 270\n",
      "clip_13762.mp4 276\n",
      "clip_49321.mp4 270\n",
      "clip_40145.mp4 276\n",
      "clip_40796.mp4 276\n",
      "clip_29459.mp4 276\n",
      "clip_2099.mp4 234\n",
      "clip_46857.mp4 276\n",
      "clip_45796.mp4 276\n",
      "clip_15280.mp4 276\n",
      "clip_27308.mp4 276\n",
      "clip_22227.mp4 276\n",
      "clip_29864.mp4 276\n",
      "clip_8517.mp4 276\n",
      "clip_40302.mp4 276\n",
      "clip_33445.mp4 276\n",
      "clip_19682.mp4 276\n",
      "clip_42342.mp4 276\n",
      "clip_19994.mp4 246\n",
      "clip_19125.mp4 276\n",
      "clip_32450.mp4 276\n",
      "clip_36867.mp4 276\n",
      "clip_13583.mp4 276\n",
      "clip_53.mp4 207\n",
      "clip_30007.mp4 276\n",
      "clip_38953.mp4 276\n",
      "clip_10433.mp4 276\n",
      "clip_48703.mp4 368\n",
      "clip_6237.mp4 276\n",
      "clip_11553.mp4 276\n",
      "clip_5111.mp4 276\n",
      "clip_43451.mp4 276\n",
      "clip_28776.mp4 276\n",
      "clip_1353.mp4 368\n",
      "clip_42399.mp4 276\n",
      "clip_3545.mp4 234\n",
      "clip_21420.mp4 260\n",
      "clip_41541.mp4 276\n",
      "clip_18414.mp4 276\n",
      "clip_48279.mp4 276\n",
      "clip_28121.mp4 276\n",
      "clip_17304.mp4 276\n",
      "clip_33435.mp4 276\n",
      "clip_49476.mp4 270\n",
      "clip_42766.mp4 276\n",
      "clip_20838.mp4 276\n",
      "clip_47924.mp4 276\n",
      "clip_42401.mp4 276\n",
      "clip_48103.mp4 276\n",
      "clip_8907.mp4 276\n",
      "clip_7016.mp4 276\n",
      "clip_22347.mp4 282\n",
      "clip_15400.mp4 276\n",
      "clip_39074.mp4 234\n",
      "clip_49096.mp4 270\n",
      "clip_36819.mp4 276\n",
      "clip_12722.mp4 276\n",
      "clip_43582.mp4 276\n",
      "clip_43769.mp4 276\n",
      "clip_8330.mp4 276\n",
      "clip_3434.mp4 273\n",
      "clip_15714.mp4 276\n",
      "clip_37383.mp4 276\n",
      "clip_14565.mp4 276\n",
      "clip_31609.mp4 276\n",
      "clip_7390.mp4 276\n",
      "clip_52046.mp4 270\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.io import read_video\n",
    "directory = \"/home/henry/Documents/PythonProjects/picklebotdataset/test_all_together/\"\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    video = read_video(directory+file)[0]\n",
    "    if video.shape[0] >= 200:\n",
    "        print(file,video.shape[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in movinet: 4660762\n",
      "number of parameters in mobilenet large: 4191584\n",
      "number of parameters in mobilenet small: 1672816\n"
     ]
    }
   ],
   "source": [
    "from movinet import MoViNetA2\n",
    "from mobilenet import MobileNetLarge3D\n",
    "movinet = MoViNetA2()\n",
    "mobilenet_large = MobileNetLarge3D()\n",
    "mobilenet_small = MobileNetSmall3D()\n",
    "\n",
    "movinet_params = sum(p.numel() for p in movinet.parameters())\n",
    "mobilenet_large_params = sum(p.numel() for p in mobilenet_large.parameters())\n",
    "mobilenet_small_params = sum(p.numel() for p in mobilenet_small.parameters())\n",
    "print(f\"number of parameters in movinet: {movinet_params}\")\n",
    "print(f\"number of parameters in mobilenet large: {mobilenet_large_params}\")\n",
    "print(f\"number of parameters in mobilenet small: {mobilenet_small_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
