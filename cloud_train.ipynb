{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10773it [2:45:38,  1.08it/s]\n",
      "100%|██████████| 1348/1348 [19:43<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss:  0.6944, val loss: 0.7042\n",
      "step 0: train accuracy:  50.03%, val accuracy: 49.26%\n",
      "Iter [1/100] - Elapsed Time: 9938.67s  Remaining Time: [993867.12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:34:12,  1.16it/s]\n",
      "100%|██████████| 1348/1348 [19:51<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss:  0.6945, val loss: 0.6936\n",
      "step 1: train accuracy:  50.06%, val accuracy: 49.98%\n",
      "Iter [2/100] - Elapsed Time: 20375.11s  Remaining Time: [1008568.08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:32:50,  1.17it/s]\n",
      "100%|██████████| 1348/1348 [19:46<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2: train loss:  0.6939, val loss: 0.6967\n",
      "step 2: train accuracy:  50.23%, val accuracy: 49.98%\n",
      "Iter [3/100] - Elapsed Time: 30737.18s  Remaining Time: [1004081.09]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "10773it [2:32:16,  1.18it/s]\n",
      " 23%|██▎       | 307/1348 [04:41<15:53,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interrupt,\n",
      "Final train loss: 0.6936, \n",
      "Final val loss: 0.6967, \n",
      "Final train accuracy: 50.18%, \n",
      "Final val accuracy: 49.98%\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "'''using torchvision for dataloading'''\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D,MobileNetLarge3D\n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy, average_for_plotting\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#hyperparameters\n",
    "torch.manual_seed(1234)\n",
    "learning_rate = 3e-4 #we use cosine annealing so this is just a starting point\n",
    "batch_size = 2 #the paper quotes 128 images/chip, but with video we have to change this\n",
    "max_iters = 100\n",
    "eval_interval = 1\n",
    "weight_decay = 5e-4\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "use_autocast = False \n",
    "compile = False\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/workspace/picklebotdataset/train'\n",
    "val_video_paths = '/workspace/picklebotdataset/val'\n",
    "\n",
    "#annotations paths\n",
    "train_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/train_labels.csv'\n",
    "val_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/val_labels.csv'\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/train_all_together'\n",
    "val_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/val_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "#dataset     \n",
    "train_dataset = PicklebotDataset(train_annotations_file,train_video_paths,transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "val_dataset = PicklebotDataset(val_annotations_file,val_video_paths,transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "#define model, initialize weights \n",
    "model = MobileNetLarge3D()\n",
    "# model.initialize_weights()\n",
    "model = model.to(device)\n",
    "\n",
    "#for multi-gpu\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# optimizer\n",
    "# optimizer = optim.RMSprop(params=model.parameters(),lr=learning_rate,weight_decay=weight_decay,momentum=momentum,eps=eps) #starting with AdamW for now. \n",
    "optimizer = optim.AdamW(params=model.parameters(),lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#cosine annealing\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "#loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_autocast:\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "model_name = model.__class__.__name__\n",
    "writer = SummaryWriter(f'runs/{model_name}') #tensorboard writer \n",
    "# checkpoint = torch.load('checkpoints/MobileNetLarge3D17.pth')\n",
    "# loaded_state_dict_keys = checkpoint.keys()\n",
    "# updated_state_dict = {}\n",
    "# for key,value in checkpoint.items():\n",
    "#     new_key = key.replace('_orig_mod.','') #remove the prefix\n",
    "#     updated_state_dict[new_key] = value\n",
    "# model.load_state_dict(updated_state_dict)\n",
    "\n",
    "\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2 and a modern gpu, these lines are from karpathy\n",
    "    print(\"compilation complete!\")\n",
    "\n",
    "\n",
    "#estimate loss using the val set, and calculate accuracy\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    val_losses = [] \n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for val_features,val_labels in tqdm(val_loader):\n",
    "        val_features = val_features.to(device)\n",
    "        val_labels = val_labels.long() #waiting to move to device until after forward pass, idk if this matters\n",
    "        # val_labels = val_labels.expand(val_features.shape[2]) #this is only for our lstm T -> batch size, a lame hack    \n",
    "        val_outputs = model(val_features)\n",
    "\n",
    "        val_loss = criterion(val_outputs,val_labels.to(device))\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        val_correct += calculate_accuracy(val_outputs,val_labels)\n",
    "        val_samples += len(val_labels)\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "#try except block so we can manually early stop while saving the model\n",
    "#training loop\n",
    "start_time = time.time()\n",
    "train_losses = torch.tensor([])\n",
    "train_percent = torch.tensor([])\n",
    "val_losses = []\n",
    "val_percent = []\n",
    "counter = 0\n",
    "\n",
    "try:\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        batch_loss_list = []\n",
    "        batch_percent_list = []\n",
    "\n",
    "        #forward pass\n",
    "        for batch_idx, (features,labels) in tqdm(enumerate(train_loader)):\n",
    "            labels = labels.to(torch.int64)\n",
    "            features = features.to(device)\n",
    "            # labels = labels.expand(features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "            \n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if use_autocast:    \n",
    "                with autocast():\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs,labels.to(device))\n",
    "                \n",
    "                #backprop & update weights\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs,labels.to(device))\n",
    "\n",
    "                #backprop & update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "\n",
    "            batch_loss_list.append(loss.item()) #append the loss of the batch to our list to be averaged and plotted later, this is dataset size / batch size long\n",
    "            batch_correct = calculate_accuracy(outputs,labels) #number of correct predictions in the batch\n",
    "            train_correct += batch_correct #this is the total number of correct predictions so far\n",
    "            train_samples += len(labels) #this is the total number of samples so far\n",
    "            batch_percent_list.append(train_correct/train_samples)\n",
    "            writer.add_scalar('training loss', batch_loss_list[-1], counter)\n",
    "            writer.add_scalar('training accuracy', batch_percent_list[-1], counter)\n",
    "            counter += 1\n",
    "\n",
    "        scheduler.step()\n",
    "        train_losses = torch.cat((train_losses,average_for_plotting(batch_loss_list))) #train losses is a tensor\n",
    "        train_percent = torch.cat((train_percent,average_for_plotting(batch_percent_list))) #train percent is a tensor\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_iters = max_iters - iter\n",
    "        avg_time_per_iter = elapsed / (iter + 1)\n",
    "        estimated_remaining_time = remaining_iters * avg_time_per_iter\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                        \n",
    "            #evaluate the model\n",
    "            val_loss, val_accuracy = estimate_loss()\n",
    "        \n",
    "            val_losses.append(val_loss) #average loss of the val dataset, this is a scalar\n",
    "            val_percent.append(val_accuracy) #percent of correct predictions in the val set, this is a scalar\n",
    "\n",
    "\n",
    "            print(f\"step {iter}: train loss:  {train_losses[-1].mean().item():.4f}, val loss: {val_losses[-1]:.4f}\") #report the average loss of the batch\n",
    "            print(f\"step {iter}: train accuracy:  {(train_percent[-1].mean().item())*100:.2f}%, val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "            writer.add_scalar('val loss', val_losses[-1], iter)\n",
    "            writer.add_scalar('val accuracy',val_percent[-1], iter)\n",
    "            torch.save(model.state_dict(), f'checkpoints/{model_name}{iter}.pth')\n",
    "\n",
    "        tqdm.write(f\"Iter [{iter+1}/{max_iters}] - Elapsed Time: {elapsed:.2f}s  Remaining Time: [{estimated_remaining_time:.2f}]\")\n",
    "        if iter == max_iters -1:\n",
    "            print(\"Training completed:\") \n",
    "            print(f\"Final train loss: {train_losses[-1].mean().item():.4f},\")\n",
    "            print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "            print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "            print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Keyboard interrupt,\\nFinal train loss: {train_losses[-1].mean().item():.4f}, \")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "    print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "    print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), f'checkpoints/{model_name}_finished.pth')\n",
    "    with open(f'statistics/{model_name}_finished_train_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_losses))\n",
    "    with open(f'statistics/{model_name}_finished_val_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_losses))\n",
    "    with open(f'statistics/{model_name}_finished_train_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_percent))\n",
    "    with open(f'statistics/{model_name}_finished_val_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_percent))\n",
    "    print(f\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This version of the program uses Nvidia Dali to load data, not torchvision.io.read_video,\n",
    "   It should be substantially faster, especially with multiple gpus, perhaps a good setup \n",
    "   would be one to load the videos, one to run the training loop? Perhaps not as I learned more about it.\n",
    "\n",
    "    Eventually, this and the other version in this notebook should be merged into one notebook, with a flag to choose which to use.\n",
    "   \n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D, MobileNetLarge3D\n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy, video_pipeline, average_for_plotting\n",
    "\n",
    "'''\n",
    "Our mean is ([0.3939, 0.3817, 0.3314])\n",
    "Our std is ([0.2104, 0.1986, 0.1829])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''Strikes are 0, balls 1.'''\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "#hyperparameters\n",
    "learning_rate = 3e-4 #the paper quotes rmsprop with 0.1 lr, but we have a tiny batch size, and are using AdamW\n",
    "batch_size = 64 #the paper quotes 128 images/chip, but with video we have to change this\n",
    "max_iters = 100\n",
    "eval_interval = 1\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "eps = np.sqrt(0.002) #From the pytorch blog post, \"a reasonable approximation can be taken with the formula PyTorch_eps = sqrt(TF_eps).\"\n",
    "std = torch.tensor([0.2104, 0.1986, 0.1829])[None,None,None,:]\n",
    "mean = torch.tensor([0.3539, 0.3817, 0.3314])[None,None,None,:]\n",
    "use_autocast = True\n",
    "compile = False\n",
    "\n",
    "#information for the dali pipeline\n",
    "sequence_length = 130 #longest videos in our dataset \n",
    "initial_prefetch_size = 20\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/workspace/picklebotdataset/train'\n",
    "val_video_paths = '/workspace/picklebotdataset/val'\n",
    "\n",
    "num_train_videos = len(os.listdir(train_video_paths + '/' + 'balls')) + len(os.listdir(train_video_paths + '/' + 'strikes'))\n",
    "num_val_videos = len(os.listdir(val_video_paths + '/' + 'balls')) + len(os.listdir(val_video_paths + '/' + 'strikes'))\n",
    "\n",
    "#define our model, initialize weights\n",
    "model = MoViNetA2()\n",
    "model.initialize_weights()\n",
    "model = model.to(device)\n",
    "\n",
    "#for multi-gpu setups \n",
    "#may want to revisit this and choose which device we use for loading with dali, and which to use for training the net.\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "#define our optimizer\n",
    "#optimizer = optim.RMSprop(params=model.parameters(),lr=learning_rate,weight_decay=weight_decay,momentum=momentum,eps=eps) #starting with AdamW for now. \n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#cosine annealing\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "#loss\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "if use_autocast:\n",
    "    scaler = GradScaler()\n",
    "model_name = model.__class__.__name__ \n",
    "writer = SummaryWriter(f'runs/{model_name}') #tensorboard writer\n",
    "# model.load_state_dict(torch.load(f'{model_name}.pth')) #if applicable, load the model from the last checkpoint\n",
    "\n",
    "\n",
    "if compile:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2 and a modern gpu, these lines were lifted from karpathy\n",
    "    print(\"compilation complete!\")\n",
    "\n",
    "#estimate_loss using validation set, we should refactor this.\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_correct = 0\n",
    "    val_samples = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for _,val_features in tqdm(enumerate(val_loader)):\n",
    "        val_labels = (val_features[0]['label']).view(-1).long() #need this as a (batch_size,) tensor\n",
    "        val_features = val_features[0]['data']/255\n",
    "        val_features = val_features.permute(0,-1,1,2,3) \n",
    "        # val_labels = val_labels.expand(val_features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "\n",
    "        val_outputs = model(val_features)\n",
    "        \n",
    "        val_loss = criterion(val_outputs,val_labels)\n",
    "        \n",
    "        val_losses.append(val_loss.item())  \n",
    "        \n",
    "        val_correct += calculate_accuracy(val_outputs,val_labels) #get number of correct\n",
    "        val_samples += len(labels) #this is the total number of samples so far\n",
    "\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    val_accuracy = val_correct / val_samples\n",
    "    return avg_val_loss, val_accuracy\n",
    "\n",
    "\n",
    "#initialize lists for plotting\n",
    "start_time = time.time()\n",
    "train_losses = torch.tensor([])\n",
    "train_percent = torch.tensor([])\n",
    "val_losses = []\n",
    "val_percent = []\n",
    "counter = 0\n",
    "\n",
    "#build our pipelines\n",
    "train_pipe = video_pipeline(batch_size=batch_size, num_threads=cpu_count(), device_id=0, file_root=train_video_paths,\n",
    "                            sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "val_pipe = video_pipeline(batch_size=batch_size, num_threads=cpu_count(), device_id=0, file_root=val_video_paths,\n",
    "                          sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "\n",
    "train_pipe.build()\n",
    "val_pipe.build()\n",
    "\n",
    "\n",
    "train_loader = DALIClassificationIterator(train_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_train_videos)\n",
    "val_loader = DALIClassificationIterator(val_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_val_videos)\n",
    "\n",
    "try:\n",
    "    for iter in range(max_iters):\n",
    "        \n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        batch_loss_list = [] #want to overwrite this each epoch\n",
    "        batch_percent_list = []\n",
    "\n",
    "        #forward pass\n",
    "        for batch_idx, features in tqdm(enumerate(train_loader)):\n",
    "            \n",
    "            labels = (features[0]['label']).view(-1).long() #need this as a (batch_size,) tensor in int64\n",
    "            features = features[0]['data']/255 #i think it makes sense to overwrite features to save precious gpu memory\n",
    "            features = features.permute(0,-1,1,2,3) #reshape for our 3D convolutions\n",
    "            # labels = labels.expand(features.shape[2]) #this is only for our lstm T -> batch size, a lame hack\n",
    "            \n",
    "            #zero the gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            if use_autocast:    \n",
    "                with autocast(dtype=dtype):\n",
    "                    outputs = model(features)\n",
    "                    loss = criterion(outputs,labels)\n",
    "                \n",
    "                #backprop & update weights\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs,labels)\n",
    "\n",
    "                #backprop & update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            #step the scheduler after the epoch\n",
    "            scheduler.step()\n",
    "            batch_loss_list.append(loss.item()) #append the loss of the batch to our list to be averaged and plotted later, this is dataset size / batch size long\n",
    "            batch_correct = calculate_accuracy(outputs,labels) #number of correct predictions in the batch\n",
    "            train_correct += batch_correct #this is the total number of correct predictions so far\n",
    "            train_samples += len(labels) #this is the total number of samples so far\n",
    "            batch_percent_list.append(train_correct/train_samples)\n",
    "            writer.add_scalar('training loss', batch_loss_list[-1], counter)\n",
    "            writer.add_scalar('training accuracy', batch_percent_list[-1], counter)\n",
    "            counter += 1\n",
    "\n",
    "        train_losses = torch.cat((train_losses,average_for_plotting(batch_loss_list))) #train losses is a tensor\n",
    "        train_percent = torch.cat((train_percent,average_for_plotting(batch_percent_list))) #train percent is a tensor\n",
    "        elapsed = time.time() - start_time\n",
    "        remaining_iters = max_iters - iter\n",
    "        avg_time_per_iter = elapsed / (iter + 1)\n",
    "        estimated_remaining_time = remaining_iters * avg_time_per_iter\n",
    "\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "                        \n",
    "            #evaluate the model\n",
    "            val_loss, val_accuracy = estimate_loss()\n",
    "        \n",
    "            val_losses.append(val_loss) #average loss of the val dataset, this is a scalar\n",
    "            val_percent.append(val_accuracy) #percent of correct predictions in the val set, this is a scalar\n",
    "\n",
    "\n",
    "            print(f\"step {iter}: train loss:  {train_losses[-1].mean().item():.4f}, val loss: {val_losses[-1]:.4f}\") #report the average loss of the batch\n",
    "            print(f\"step {iter}: train accuracy:  {(train_percent[-1].mean().item())*100:.2f}%, val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "            writer.add_scalar('val loss', val_losses[-1], iter)\n",
    "            writer.add_scalar('val accuracy',val_percent[-1], iter)\n",
    "            torch.save(model.state_dict(), f'checkpoints/{model_name}{iter}.pth')\n",
    "\n",
    "        tqdm.write(f\"Iter [{iter+1}/{max_iters}] - Elapsed Time: {elapsed:.2f}s  Remaining Time: [{estimated_remaining_time:.2f}]\")\n",
    "        if iter == max_iters -1:\n",
    "            print(\"Training completed:\") \n",
    "            print(f\"Final train loss: {train_losses[-1].mean().item():.4f},\")\n",
    "            print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "            print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "            print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"Keyboard interrupt,\\nFinal train loss: {train_losses[-1].mean().item():.4f}, \")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}, \")\n",
    "    print(f\"Final train accuracy: {(train_percent[-1].mean().item())*100:.2f}%, \")\n",
    "    print(f\"Final val accuracy: {val_percent[-1]*100:.2f}%\")\n",
    "\n",
    "finally:\n",
    "    torch.save(model.state_dict(), f'checkpoints/{model_name}_finished.pth')\n",
    "    with open(f'statistics/{model_name}_finished_train_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_losses))\n",
    "    with open(f'statistics/{model_name}_finished_val_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_losses))\n",
    "    with open(f'statistics/{model_name}_finished_train_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(train_percent))\n",
    "    with open(f'statistics/{model_name}_finished_val_percent.npy', 'wb') as f:\n",
    "        np.save(f, np.array(val_percent))\n",
    "    print(f\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1347/1347 [20:14<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenet small test loss: 0.5058, mobilenet small test accuracy: 80.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''For testing our network'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D,MobileNetLarge3D\n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    model.eval()\n",
    "    test_losses = [] \n",
    "    test_correct = 0\n",
    "    test_samples = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for test_features,test_labels in tqdm(test_loader):\n",
    "        test_features = test_features.to(device)\n",
    "        test_labels = test_labels.to(torch.int64) #waiting to move to device until after forward pass, idk if this matters\n",
    "        # val_labels = val_labels.expand(val_features.shape[2]) #this is only for our lstm T -> batch size, a lame hack    \n",
    "        test_outputs = model(test_features)\n",
    "\n",
    "        test_loss = criterion(test_outputs,test_labels.to(device))\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        test_correct += calculate_accuracy(test_outputs,test_labels)\n",
    "        test_samples += len(test_labels)\n",
    "\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    test_accuracy = test_correct / test_samples\n",
    "    return avg_test_loss, test_accuracy\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "batch_size = 4 \n",
    "\n",
    "#annotations paths\n",
    "test_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/test_labels.csv'\n",
    "\n",
    "#video paths\n",
    "test_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/test_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "#dataset     \n",
    "test_dataset = PicklebotDataset(test_annotations_file,test_video_paths,transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "model = MobileNetSmall3D()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.load_state_dict(torch.load(f'models/mobilenet_small.pth'))\n",
    "model.to(device)\n",
    "avg_test_loss,test_accuracy = estimate_loss()\n",
    "\n",
    "print(f'mobilenet small test loss: {avg_test_loss:.4f}, mobilenet small test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in movinet: 4660762\n",
      "number of parameters in mobilenet large: 4191584\n",
      "number of parameters in mobilenet small: 1672816\n"
     ]
    }
   ],
   "source": [
    "'''Calculate the number of parameters in each model, for comparison purposes. \n",
    "   Note that movinet is about 2.8x larger than mobilenet small, and mobilenet large is about 2.5x larger than mobilenet small.'''\n",
    "\n",
    "from movinet import MoViNetA2\n",
    "from mobilenet import MobileNetLarge3D\n",
    "movinet = MoViNetA2()\n",
    "mobilenet_large = MobileNetLarge3D()\n",
    "mobilenet_small = MobileNetSmall3D()\n",
    "\n",
    "movinet_params = sum(p.numel() for p in movinet.parameters())\n",
    "mobilenet_large_params = sum(p.numel() for p in mobilenet_large.parameters())\n",
    "mobilenet_small_params = sum(p.numel() for p in mobilenet_small.parameters())\n",
    "print(f\"number of parameters in movinet: {movinet_params}\")\n",
    "print(f\"number of parameters in mobilenet large: {mobilenet_large_params}\")\n",
    "print(f\"number of parameters in mobilenet small: {mobilenet_small_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
