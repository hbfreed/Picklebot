{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 52/811 [00:34<08:18,  1.52it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m\n\u001b[1;32m     77\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PicklebotDataset(annotations_file,video_paths,dtype\u001b[38;5;241m=\u001b[39mdtype,backend\u001b[38;5;241m=\u001b[39mbe)\n\u001b[1;32m     78\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate,num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataloader import custom_collate, PicklebotDataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "annotations_file = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_val.csv'\n",
    "video_paths = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_all_together'\n",
    "batch_size = 16\n",
    "\n",
    "class PicklebotDataset(Dataset):\n",
    "    def __init__(self, annotations_file, video_dir, transform=None, target_transform=None, dtype=torch.bfloat16, backend='opencv'):\n",
    "        self.video_labels = pd.read_csv(annotations_file, engine='pyarrow', encoding='ISO-8859-1')\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dtype = dtype\n",
    "        self.backend = backend\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_labels.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_labels['filename'][idx])\n",
    "\n",
    "        if self.backend == 'newcv':\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1024)\n",
    "\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "            frames = np.empty((frame_count, frame_height, frame_width,3), dtype=np.uint8) #channel last so we only have to permute once at the end\n",
    "\n",
    "            for i in range(frame_count):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames[i] = frame\n",
    "\n",
    "            cap.release()\n",
    "            video = torch.from_numpy(frames).permute(0,3,1,2).to(self.dtype) / 255\n",
    "\n",
    "        elif self.backend == 'opencv':\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "            video = torch.stack(frames).to('cuda').permute(0,3,1,2).to(self.dtype)/255\n",
    "\n",
    "        label = self.video_labels[\"zone\"][idx]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return video, label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for be in ['newcv','opencv']:\n",
    "    #start the timer\n",
    "    start = time.time()    \n",
    "    dataset = PicklebotDataset(annotations_file,video_paths,dtype=dtype,backend=be)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,shuffle=False,collate_fn=custom_collate,num_workers=8,pin_memory=True)\n",
    "\n",
    "\n",
    "    for i in tqdm(loader):\n",
    "        pass\n",
    "    print(f'time for {be}:',time.time()-start)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.54661 seconds\n",
      "GPU time: 0.00087 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Create a large tensor\n",
    "tensor_cpu = torch.rand(50000, 50000)\n",
    "tensor_gpu = tensor_cpu.to('cuda')\n",
    "\n",
    "# Perform pointwise operation on CPU\n",
    "start_cpu = time.time()\n",
    "result_cpu = tensor_cpu / 255\n",
    "end_cpu = time.time()\n",
    "cpu_time = end_cpu - start_cpu\n",
    "\n",
    "# Perform pointwise operation on GPU\n",
    "start_gpu = time.time()\n",
    "result_gpu = tensor_gpu / 255\n",
    "end_gpu = time.time()\n",
    "gpu_time = end_gpu - start_gpu\n",
    "\n",
    "print(f\"CPU time: {cpu_time:.5f} seconds\")\n",
    "print(f\"GPU time: {gpu_time:.5f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for 100 small permutes: 0.2436676025390625\n",
      "time for one big permute: 0.19919157028198242\n"
     ]
    }
   ],
   "source": [
    "#for numpy.permute, is it faster to do a ton of small permutes or one big one at the end?\n",
    "#we test it here:\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "a = np.random.rand(1000,224,224,3)\n",
    "b = np.empty((1000,3,224,224))\n",
    "start = time.time()\n",
    "for i in range(len(a)):\n",
    "    b[i] = a[i].transpose(2,0,1)\n",
    "print('time for 100 small permutes:',time.time()-start)\n",
    "\n",
    "a = np.random.rand(1000,224,224,3)\n",
    "b = np.empty((1000,224,224,3))\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    b[i] = a[i]\n",
    "b = b.transpose(0,2,3,1)\n",
    "print('time for one big permute:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all batch_size=16\n",
    "#num_workers = 4\n",
    "# 100%|██████████| 6483/6483 [1:02:53<00:00,  1.72it/s]time for opencv: 3773.560480117798\n",
    "\n",
    "#num_workers = 6\n",
    "\n",
    "\n",
    "\n",
    "# num_workers = 8,\n",
    "\n",
    "# 100%|██████████| 6483/6483 [45:50<00:00,  2.36it/s]  \n",
    "# time for torchvision: 2751.082986831665\n",
    "# 100%|██████████| 6483/6483 [39:01<00:00,  2.77it/s] \n",
    "# time for opencv: 2341.418078660965\n",
    "\n",
    "# num_workers = 12 (yes, opencv is practically the same!)\n",
    "# 100%|██████████| 6483/6483 [39:31<00:00,  2.73it/s] \n",
    "# time for torchvision: 2372.112513780594\n",
    "# 100%|██████████| 6483/6483 [39:01<00:00,  2.77it/s] \n",
    "# time for opencv: 2341.3904235363007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4388841\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTV2Config, MobileViTV2Model\n",
    "import torch\n",
    "\n",
    "# Initializing a mobilevitv2-small style configuration\n",
    "configuration = MobileViTV2Config()\n",
    "\n",
    "# Initializing a model from the mobilevitv2-small style configuration\n",
    "model = MobileViTV2Model(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "9471273\n"
     ]
    }
   ],
   "source": [
    "from mobilevit import MobileViTV2\n",
    "import torch\n",
    "ownmodel = MobileViTV2(num_classes=512)\n",
    "A = torch.rand(2, 3, 256, 256)\n",
    "print(ownmodel(A).shape)\n",
    "print(sum(p.numel() for p in ownmodel.parameters()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileViTV2Model(\n",
       "  (conv_stem): MobileViTV2ConvLayer(\n",
       "    (convolution): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (normalization): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): SiLU()\n",
       "  )\n",
       "  (encoder): MobileViTV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): MobileViTV2MobileNetLayer(\n",
       "        (layer): ModuleList(\n",
       "          (0): MobileViTV2InvertedResidual(\n",
       "            (expand_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (conv_3x3): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (reduce_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MobileViTV2MobileNetLayer(\n",
       "        (layer): ModuleList(\n",
       "          (0): MobileViTV2InvertedResidual(\n",
       "            (expand_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (conv_3x3): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "              (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (reduce_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): MobileViTV2InvertedResidual(\n",
       "            (expand_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (conv_3x3): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): SiLU()\n",
       "            )\n",
       "            (reduce_1x1): MobileViTV2ConvLayer(\n",
       "              (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MobileViTV2Layer(\n",
       "        (downsampling_layer): MobileViTV2InvertedResidual(\n",
       "          (expand_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (conv_3x3): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (reduce_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_kxk): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "          (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (conv_1x1): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (transformer): MobileViTV2Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-1): 2 x MobileViTV2TransformerLayer(\n",
       "              (layernorm_before): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "              (attention): MobileViTV2LinearSelfAttention(\n",
       "                (qkv_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(128, 257, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (out_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (layernorm_after): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "              (ffn): MobileViTV2FFN(\n",
       "                (conv1): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                )\n",
       "                (dropout1): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (dropout2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        (conv_projection): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): MobileViTV2Layer(\n",
       "        (downsampling_layer): MobileViTV2InvertedResidual(\n",
       "          (expand_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (conv_3x3): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "            (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (reduce_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_kxk): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (conv_1x1): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (transformer): MobileViTV2Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-3): 4 x MobileViTV2TransformerLayer(\n",
       "              (layernorm_before): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "              (attention): MobileViTV2LinearSelfAttention(\n",
       "                (qkv_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(192, 385, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (out_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (layernorm_after): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "              (ffn): MobileViTV2FFN(\n",
       "                (conv1): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                )\n",
       "                (dropout1): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (dropout2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): GroupNorm(1, 192, eps=1e-05, affine=True)\n",
       "        (conv_projection): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): MobileViTV2Layer(\n",
       "        (downsampling_layer): MobileViTV2InvertedResidual(\n",
       "          (expand_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (conv_3x3): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=768, bias=False)\n",
       "            (normalization): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (activation): SiLU()\n",
       "          )\n",
       "          (reduce_1x1): MobileViTV2ConvLayer(\n",
       "            (convolution): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv_kxk): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "          (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): SiLU()\n",
       "        )\n",
       "        (conv_1x1): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (transformer): MobileViTV2Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-2): 3 x MobileViTV2TransformerLayer(\n",
       "              (layernorm_before): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "              (attention): MobileViTV2LinearSelfAttention(\n",
       "                (qkv_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(256, 513, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (out_proj): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "              )\n",
       "              (dropout1): Dropout(p=0.0, inplace=False)\n",
       "              (layernorm_after): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "              (ffn): MobileViTV2FFN(\n",
       "                (conv1): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                )\n",
       "                (dropout1): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): MobileViTV2ConvLayer(\n",
       "                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                )\n",
       "                (dropout2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        (conv_projection): MobileViTV2ConvLayer(\n",
       "          (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8019bb7d72364b21a8aff81811e4faca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imagenet-1k\", split=\"train\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import os\n",
    "import torch\n",
    "#information for the dali pipeline\n",
    "sequence_length = 130 #longest videos in our dataset \n",
    "initial_prefetch_size = 20\n",
    "batch_size = 16\n",
    "\n",
    "@pipeline_def\n",
    "def dali_video_pipeline(filenames, sequence_length, initial_prefetch_size,mean,std):\n",
    "    videos, labels = fn.experimental.readers.video(device=\"cpu\", filenames=filenames, sequence_length=sequence_length,\n",
    "                              shard_id=0, num_shards=1, random_shuffle=False, initial_fill=initial_prefetch_size)\n",
    "    videos = fn.normalize(videos,mean=mean,stddev=std)\n",
    "    return videos, labels\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/train_all_together'\n",
    "val_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/val_all_together'\n",
    "train_files = [f\"{train_video_paths}/{video}\" for video in os.listdir(train_video_paths)]\n",
    "val_files = [f\"{val_video_paths}/{video}\" for video in os.listdir(val_video_paths)]\n",
    "\n",
    "\n",
    "num_train_videos = len(train_files)\n",
    "num_val_videos = len(val_files)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "#multiply mean and val by 255 to convert to 0-255 range\n",
    "mean = (torch.tensor(mean)*255)[None,None,None,:]\n",
    "std = (torch.tensor(std)*255)[None,None,None,:]\n",
    "\n",
    "print(\"Building DALI pipelines...\")\n",
    "\n",
    "#build our pipelines\n",
    "train_pipe = dali_video_pipeline(batch_size=batch_size, num_threads=24, device_id=None, filenames=train_files,sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean*255,std=std*255)\n",
    "# val_pipe = dali_video_pipeline(batch_size=batch_size, num_threads=24, device_id=None, filenames=val_files, sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "\n",
    "train_pipe.build()\n",
    "# val_pipe.build()\n",
    "\n",
    "\n",
    "train_loader = DALIClassificationIterator(train_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_train_videos)\n",
    "val_loader = DALIClassificationIterator(val_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_val_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5005/5005 [12:06<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dali time: 726.4026355743408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5005 [00:00<?, ?it/s]/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "/tmp/ipykernel_19220/1915391475.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  tensor[i] += torch.from_numpy(nump_array)\n",
      "100%|██████████| 5005/5005 [04:14<00:00, 19.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch time: 254.73878645896912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def custom_collate(batch, memory_format):\n",
    "    \"\"\"Based on fast_collate from the APEX example\n",
    "       https://github.com/NVIDIA/apex/blob/5b5d41034b506591a316c308c3d2cd14d5187e23/examples/imagenet/main_amp.py#L265\n",
    "    \"\"\"\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].size[0]\n",
    "    h = imgs[0].size[1]\n",
    "    tensor = torch.zeros( (len(imgs), 3, h, w), dtype=torch.uint8).contiguous(memory_format=memory_format)\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.asarray(img, dtype=np.uint8)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "    return tensor, targets\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def create_dali_pipeline(data_dir, crop, size, shard_id, num_shards, dali_cpu=True, is_training=True):\n",
    "    images, labels = fn.readers.file(file_root=data_dir,\n",
    "                                     shard_id=shard_id,\n",
    "                                     num_shards=num_shards,\n",
    "                                     random_shuffle=is_training,\n",
    "                                     pad_last_batch=True,\n",
    "                                     name=\"Reader\")\n",
    "    dali_device = 'cpu' if dali_cpu else 'gpu'\n",
    "    decoder_device = 'cpu' if dali_cpu else 'mixed'\n",
    "    # ask nvJPEG to preallocate memory for the biggest sample in ImageNet for CPU and GPU to avoid reallocations in runtime\n",
    "    device_memory_padding = 211025920 if decoder_device == 'mixed' else 0\n",
    "    host_memory_padding = 140544512 if decoder_device == 'mixed' else 0\n",
    "    # ask HW NVJPEG to allocate memory ahead for the biggest image in the data set to avoid reallocations in runtime\n",
    "    preallocate_width_hint = 5980 if decoder_device == 'mixed' else 0\n",
    "    preallocate_height_hint = 6430 if decoder_device == 'mixed' else 0\n",
    "    if is_training:\n",
    "        images = fn.decoders.image_random_crop(images,\n",
    "                                               device=decoder_device, output_type=types.RGB,\n",
    "                                               device_memory_padding=device_memory_padding,\n",
    "                                               host_memory_padding=host_memory_padding,\n",
    "                                               preallocate_width_hint=preallocate_width_hint,\n",
    "                                               preallocate_height_hint=preallocate_height_hint,\n",
    "                                               random_aspect_ratio=[0.8, 1.25],\n",
    "                                               random_area=[0.1, 1.0],\n",
    "                                               num_attempts=100)\n",
    "        images = fn.resize(images,\n",
    "                           device=dali_device,\n",
    "                           resize_x=crop,\n",
    "                           resize_y=crop,\n",
    "                           interp_type=types.INTERP_TRIANGULAR)\n",
    "        mirror = fn.random.coin_flip(probability=0.5)\n",
    "    else:\n",
    "        images = fn.decoders.image(images,\n",
    "                                   device=decoder_device,\n",
    "                                   output_type=types.RGB)\n",
    "        images = fn.resize(images,\n",
    "                           device=dali_device,\n",
    "                           size=size,\n",
    "                           mode=\"not_smaller\",\n",
    "                           interp_type=types.INTERP_TRIANGULAR)\n",
    "        mirror = False\n",
    "\n",
    "    images = fn.crop_mirror_normalize(images,\n",
    "                                      dtype=types.FLOAT,\n",
    "                                      output_layout=\"CHW\",\n",
    "                                      crop=(crop, crop),\n",
    "                                      mean=[0.485 * 255,0.456 * 255,0.406 * 255],\n",
    "                                      std=[0.229 * 255,0.224 * 255,0.225 * 255],\n",
    "                                      mirror=mirror)\n",
    "    labels = labels\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "crop_size = 256\n",
    "val_size = 256\n",
    "batch_size = 256\n",
    "num_threads = 24\n",
    "device_id = None\n",
    "dali_cpu = True\n",
    "\n",
    "traindir = '/home/henry/Documents/imagenet/train/'\n",
    "torch_dataset = datasets.ImageFolder(traindir, transforms.Compose([transforms.RandomResizedCrop(crop_size),transforms.RandomHorizontalFlip()]))\n",
    "torch_loader = torch.utils.data.DataLoader(torch_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_threads,\n",
    "                                            pin_memory=True,\n",
    "                                            collate_fn= lambda b: custom_collate(b,torch.channels_last))\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "train_pipe = create_dali_pipeline(batch_size=batch_size,\n",
    "                                    num_threads=num_threads,\n",
    "                                    device_id=device_id,\n",
    "                                    seed=-1,\n",
    "                                    data_dir=traindir,\n",
    "                                    crop=crop_size,\n",
    "                                    size=val_size,\n",
    "                                    dali_cpu=dali_cpu,\n",
    "                                    shard_id=0,\n",
    "                                    num_shards=1,\n",
    "                                    is_training=True)\n",
    "train_pipe.build()\n",
    "train_loader = DALIClassificationIterator(train_pipe, reader_name=\"Reader\",\n",
    "                                            last_batch_policy=LastBatchPolicy.PARTIAL,\n",
    "                                            auto_reset=True)\n",
    "\n",
    "#start the timer\n",
    "start = time.time()\n",
    "for data in tqdm(train_loader):\n",
    "    pass\n",
    "print('dali time:',time.time()-start)\n",
    "\n",
    "#start the timer\n",
    "start = time.time()\n",
    "for data in tqdm(torch_loader):\n",
    "    pass\n",
    "print('torch time:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nvidia.dali import pipeline_def\n",
    "from nvidia.dali import fn\n",
    "from nvidia.dali.plugin.pytorch import DALIClassificationIterator, LastBatchPolicy\n",
    "from psutil import cpu_count\n",
    "import pandas as pd\n",
    "#information for the dali pipeline\n",
    "sequence_length = 130 #longest videos in our dataset \n",
    "initial_prefetch_size = 1 \n",
    "batch_size = 16\n",
    "\n",
    "#video paths\n",
    "video_paths = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_all_together'\n",
    "train_df = pd.read_csv('/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_train.csv', engine='pyarrow', encoding='ISO-8859-1')\n",
    "val_df = pd.read_csv('/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_val.csv', engine='pyarrow', encoding='ISO-8859-1')\n",
    "train_files = [f\"{video_paths}/{video}\" for video in train_df['filename']]\n",
    "val_files = [f\"{video_paths}/{video}\" for video in val_df['filename']]\n",
    "train_labels = train_df['zone'].to_list()\n",
    "val_labels = val_df['zone'].to_list()\n",
    "\n",
    "\n",
    "num_train_videos = len(train_files)\n",
    "num_val_videos = len(val_files)\n",
    "\n",
    "#multiply mean and val by 255 to convert to 0-255 range\n",
    "mean = [0.485, 0.456, 0.406] \n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = (torch.tensor(mean)*255)[None,None,None,:]\n",
    "std = (torch.tensor(std)*255)[None,None,None,:]\n",
    "\n",
    "@pipeline_def\n",
    "def dali_video_pipeline(filenames, labels,sequence_length, initial_prefetch_size,mean,std):\n",
    "    videos, labels = fn.experimental.readers.video(device=\"cpu\", filenames=filenames, sequence_length=sequence_length, labels=labels,\n",
    "                              random_shuffle=False, initial_fill=initial_prefetch_size)\n",
    "    videos = fn.normalize(videos,mean=mean,stddev=std)\n",
    "    return videos, labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"Building DALI pipelines...\")\n",
    "\n",
    "#build our pipelines\n",
    "train_pipe = dali_video_pipeline(batch_size=batch_size, num_threads=cpu_count()//2, device_id=None, filenames=train_files, labels=train_labels,\n",
    "                            sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean*255,std=std*255)\n",
    "val_pipe = dali_video_pipeline(batch_size=batch_size, num_threads=cpu_count()//2, device_id=None, filenames=val_files, labels=val_labels,\n",
    "                        sequence_length=sequence_length,initial_prefetch_size=initial_prefetch_size,mean=mean,std=std)\n",
    "\n",
    "train_pipe.build()\n",
    "val_pipe.build()\n",
    "\n",
    "\n",
    "train_loader = DALIClassificationIterator(train_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_train_videos)\n",
    "val_loader = DALIClassificationIterator(val_pipe, auto_reset=True,last_batch_policy=LastBatchPolicy.PARTIAL, size=num_val_videos)\n",
    "\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from nvidia.dali import pipeline_def\n",
    "import nvidia.dali.fn as fn\n",
    "import nvidia.dali.types as types\n",
    "\n",
    "batch_size=2\n",
    "sequence_length=8\n",
    "initial_prefetch_size=16\n",
    "video_directory = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_all_together'\n",
    "video_files=[video_directory + '/' + f for f in os.listdir(video_directory)]\n",
    "n_iter=6\n",
    "\n",
    "\n",
    "@pipeline_def\n",
    "def video_pipe(filenames):\n",
    "    videos = fn.experimental.readers.video(device=\"cpu\", filenames=filenames, sequence_length=sequence_length,\n",
    "                              shard_id=None, num_shards=1, random_shuffle=True, initial_fill=initial_prefetch_size)\n",
    "    return videos\n",
    "\n",
    "\n",
    "pipe = video_pipe(batch_size=batch_size, num_threads=2, device_id=None, filenames=video_files, seed=123456)\n",
    "pipe.build()\n",
    "print('about to start loop')\n",
    "for i in range(n_iter):\n",
    "    pipe_out = pipe.run()\n",
    "    sequences_out = pipe_out[0]\n",
    "    print(sequences_out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
