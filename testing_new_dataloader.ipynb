{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2 loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 811/811 [05:54<00:00,  2.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 354.9956748485565\n",
      "old loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/811 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_video\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import custom_collate\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "val_annotations_file = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_val.csv'\n",
    "video_paths = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_all_together'\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "class cv2PicklebotDataset(Dataset):\n",
    "    def __init__(self, annotations_file, video_dir, transform=None, target_transform=None, dtype=torch.bfloat16):\n",
    "        self.video_labels = pd.read_csv(annotations_file, engine='pyarrow', encoding='ISO-8859-1')\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_labels.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_labels['filename'][idx])\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = torch.from_numpy(frame).permute(2, 0, 1).to(self.dtype) / 255\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        video = torch.stack(frames)\n",
    "        label = self.video_labels[\"zone\"][idx]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return video, label\n",
    "    \n",
    "class PicklebotDataset(Dataset):\n",
    "    def __init__(self, annotations_file, video_dir, transform=None,target_transform=None,dtype=torch.bfloat16):\n",
    "        self.video_labels = pd.read_csv(annotations_file,engine='pyarrow',encoding='ISO-8859-1')\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_labels.shape[0]\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_labels['filename'][idx])\n",
    "        video = ((read_video(video_path,output_format=\"TCHW\",pts_unit='sec')[0]).to(self.dtype))/255\n",
    "        label = self.video_labels[\"zone\"][idx]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return video, label\n",
    "    \n",
    "\n",
    "\n",
    "cv2val_dataset = cv2PicklebotDataset(val_annotations_file,video_paths,dtype=dtype)\n",
    "val_dataset = PicklebotDataset(val_annotations_file,video_paths,dtype=dtype)\n",
    "cv2val_loader = DataLoader(cv2val_dataset, batch_size=batch_size,shuffle=False,collate_fn=custom_collate,num_workers=16,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False,collate_fn=custom_collate,num_workers=16,pin_memory=True)\n",
    "\n",
    "\n",
    "#test how long it takes each loader to load the data, passing in the loop, and running both loaders twice\n",
    "for i in range(2):\n",
    "    print('cv2 loader')\n",
    "    start = time.time()\n",
    "    for batch in tqdm(cv2val_loader):\n",
    "        pass\n",
    "        batch = None\n",
    "    print('time:',time.time()-start)\n",
    "    \n",
    "    gc.collect()\n",
    "    print('old loader')\n",
    "    start = time.time()\n",
    "    for batch in tqdm(val_loader):\n",
    "        pass\n",
    "        batch = None\n",
    "    \n",
    "    print('time:',time.time()-start)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import custom_collate, PicklebotDataset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "annotations_file = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_train.csv'\n",
    "video_paths = '/home/henry/Documents/PythonProjects/picklebot_2m/picklebot_130k_all_together'\n",
    "batch_size = 16\n",
    "\n",
    "for be in ['opencv']: #,'torchvision']:\n",
    "    #start the timer\n",
    "    start = time.time()    \n",
    "    dataset = PicklebotDataset(annotations_file,video_paths,dtype=dtype,backend=be)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size,shuffle=False,collate_fn=custom_collate,num_workers=6,pin_memory=True)\n",
    "\n",
    "\n",
    "    for i in tqdm(loader):\n",
    "        pass\n",
    "    print(f'time for {be}:',time.time()-start)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all batch_size=16\n",
    "#num_workers = 4\n",
    "# 100%|██████████| 6483/6483 [1:02:53<00:00,  1.72it/s]time for opencv: 3773.560480117798\n",
    "\n",
    "#num_workers = 6\n",
    "\n",
    "\n",
    "\n",
    "# num_workers = 8,\n",
    "\n",
    "# 100%|██████████| 6483/6483 [45:50<00:00,  2.36it/s]  \n",
    "# time for torchvision: 2751.082986831665\n",
    "# 100%|██████████| 6483/6483 [39:01<00:00,  2.77it/s] \n",
    "# time for opencv: 2341.418078660965\n",
    "\n",
    "# num_workers = 12 (yes, opencv is practically the same!)\n",
    "# 100%|██████████| 6483/6483 [39:31<00:00,  2.73it/s] \n",
    "# time for torchvision: 2372.112513780594\n",
    "# 100%|██████████| 6483/6483 [39:01<00:00,  2.77it/s] \n",
    "# time for opencv: 2341.3904235363007\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape: torch.Size([2, 64, 32, 32])\n",
      "torch.Size([2, 64, 32, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [96] and input of shape [2, 64, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m B \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 50\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m A \u001b[38;5;241m=\u001b[39m attn(A)\n\u001b[1;32m     52\u001b[0m A \u001b[38;5;241m=\u001b[39m dropout(A)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1536\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1535\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1539\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/normalization.py:287\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2590\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2588\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2589\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [96] and input of shape [2, 64, 32, 32]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mobilenet import Bottleneck2D, Bottleneck3D\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Reduce\n",
    "from typing import Union, Tuple, List\n",
    "from mobilevit import MobileViTBlock, conv_nxn_bn, LinearAttention, LinearFeedForward\n",
    "init_dim = 16\n",
    "expansion = 4\n",
    "depths = (2,4,5)\n",
    "kernel_size = 3\n",
    "patch_size = (2,2)\n",
    "use_linear_attention = False\n",
    "image_size=(256,256)\n",
    "dims=[96,120,144]\n",
    "channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
    "\n",
    "conv1 = conv_nxn_bn(3, init_dim, stride=2) #3 channels to 16\n",
    "bn1 = Bottleneck2D(channels[0],channels[1],expanded_channels=channels[0]*expansion,stride=1)\n",
    "bn2 = Bottleneck2D(channels[1],channels[2],expanded_channels=channels[1]*expansion,stride=2)\n",
    "bn3 = Bottleneck2D(channels[2],channels[3],expanded_channels=channels[2]*expansion,stride=1)\n",
    "bn4 = Bottleneck2D(channels[2],channels[3],expanded_channels=channels[2]*expansion,stride=1)\n",
    "bn5 = Bottleneck2D(channels[3],channels[4],expanded_channels=channels[3]*expansion,stride=2)\n",
    "tf1 = MobileViTBlock(dims[0],depths[0],channels[5],kernel_size,patch_size, int(dims[0]*2),use_linear_attention=use_linear_attention)\n",
    "\n",
    "ln1 = nn.GroupNorm(num_groups=1,num_channels=dims[0], eps=1e-05)\n",
    "attn = LinearAttention(embed_dim=dims[0], dropout=0.)\n",
    "dropout = nn.Dropout(p=0.)\n",
    "ln2 = nn.GroupNorm(num_groups=1,num_channels=dims[0], eps=1e-05)\n",
    "ffw = LinearFeedForward(embed_dim=dims[0], ffw_dim=dims[0], dropout=0.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf2 = MobileViTBlock(dims[0],depths[0],channels[5],kernel_size,patch_size, int(dims[0]*2),use_linear_attention=True)\n",
    "\n",
    "A = torch.randn(2,3,256,256)\n",
    "\n",
    "A = conv1(A)\n",
    "\n",
    "A = bn1(A)\n",
    "A = bn2(A)\n",
    "A = bn3(A)\n",
    "A = bn4(A)\n",
    "A = bn5(A)\n",
    "A1 = tf1(A)\n",
    "B = A.clone()\n",
    "print(A.shape)\n",
    "A = ln1(A)\n",
    "A = attn(A)\n",
    "A = dropout(A)\n",
    "A = A + B\n",
    "A =ln2(A)\n",
    "A = ffw(A)\n",
    "A = A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after bottleneck 3.5: torch.Size([2, 128, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from mobilevitv2 import *\n",
    "import torch\n",
    "\n",
    "model = MobileViTV2()\n",
    "A = torch.rand(2,3,256,256)\n",
    "A = model(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
