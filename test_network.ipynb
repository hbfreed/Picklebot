{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For testing our network'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D,MobileNetLarge3D\n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy, calculate_accuracy_bce\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "def forward_pass(loader,model):\n",
    "    model.eval()\n",
    "    test_losses = [] \n",
    "    test_samples = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for test_features,test_labels in tqdm(loader):\n",
    "        test_features = test_features.to(torch.bfloat16).to(device)\n",
    "        test_labels = (test_labels.long()).to(device) \n",
    "        test_outputs = model(test_features)\n",
    "        # test_labels = test_labels.unsqueeze(1)\n",
    "        test_loss = criterion(test_outputs,test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        test_correct += calculate_accuracy(test_outputs,test_labels)\n",
    "        \n",
    "        test_samples += len(test_labels)\n",
    "        \n",
    "\n",
    "    return test_losses, test_correct, test_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    balls_losses, balls_correct, balls_samples = forward_pass(ball_loader,model)\n",
    "    strikes_losses,strikes_correct, strikes_samples = forward_pass(strike_loader,model)\n",
    "    test_losses = balls_losses + strikes_losses\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    print(balls_correct)\n",
    "    print(strikes_correct)\n",
    "    balls_accuracy = balls_correct / balls_samples \n",
    "    strikes_accuracy = strikes_correct / strikes_samples \n",
    "    return avg_test_loss, balls_accuracy, strikes_accuracy \n",
    "\n",
    "def state_dict_converter(state_dict):\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"_orig_mod.\"):\n",
    "            new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "batch_size = 4 \n",
    "#annotations paths\n",
    "strike_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/strike_test_labels.csv'\n",
    "ball_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/ball_test_labels.csv'\n",
    "\n",
    "#video paths\n",
    "test_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/test_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "\n",
    "ball_dataset = PicklebotDataset(ball_annotations_file,test_video_paths,transform=transform)\n",
    "strike_dataset = PicklebotDataset(strike_annotations_file,test_video_paths,transform=transform)\n",
    "ball_loader = DataLoader(ball_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "strike_loader = DataLoader(strike_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "\n",
    "model = MobileNetSmall3D(num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "state_dict = torch.load(f'/home/henry/Documents/PythonProjects/Picklebot/checkpoints/MobileNetSmall3D31.pth')\n",
    "\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(torch.bfloat16)\n",
    "avg_test_loss,balls_accuracy,strikes_accuracy = estimate_loss()\n",
    "print(f'Mobilenet Small test loss: {avg_test_loss:.4f}, ball test accuracy: {balls_accuracy * 100:.2f}% strike test accuracy: {strikes_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobilenet import MobileNetLarge3D\n",
    "import torch\n",
    "import os\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "def state_dict_converter(state_dict):\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"_orig_mod.\"):\n",
    "            new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    return state_dict\n",
    "\n",
    "def transform(video, mean, std):\n",
    "    return (video - mean) / std\n",
    "\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "# Convert the mean and std to tensors\n",
    "std = torch.tensor(std).view(1, 3, 1, 1, 1)\n",
    "mean = torch.tensor(mean).view(1, 3, 1, 1, 1)\n",
    "\n",
    "test_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/test/balls'\n",
    "state_dict = torch.load(f'/home/henry/Documents/PythonProjects/Picklebot/weights/MobileNetLarge.pth')\n",
    "model = MobileNetLarge3D()\n",
    "model.load_state_dict(state_dict_converter(state_dict))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "video_paths = os.listdir(test_video_paths)\n",
    "\n",
    "for video_path in video_paths:\n",
    "    video = test_video_paths + '/' + video_path\n",
    "    video = torchvision.io.read_video(video,pts_unit='sec')[0].permute(-1,0,1,2).unsqueeze(0)/255\n",
    "    video = transform(video,mean,std)\n",
    "    with autocast():\n",
    "        out = model(video)\n",
    "    out_prob = F.softmax(out,dim=1)\n",
    "    print(out_prob,video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in movinet: 4660762\n",
      "number of parameters in mobilenet large: 4191584\n",
      "number of parameters in mobilenet small: 1672816\n"
     ]
    }
   ],
   "source": [
    "'''Calculate the number of parameters in each model, for comparison purposes. \n",
    "   Note that movinet is about 2.8x larger than mobilenet small, and mobilenet large is about 2.5x larger than mobilenet small.'''\n",
    "\n",
    "from movinet import MoViNetA2\n",
    "from mobilenet import MobileNetLarge3D\n",
    "movinet = MoViNetA2()\n",
    "mobilenet_large = MobileNetLarge3D()\n",
    "mobilenet_small = MobileNetSmall3D()\n",
    "\n",
    "movinet_params = sum(p.numel() for p in movinet.parameters())\n",
    "mobilenet_large_params = sum(p.numel() for p in mobilenet_large.parameters())\n",
    "mobilenet_small_params = sum(p.numel() for p in mobilenet_small.parameters())\n",
    "print(f\"number of parameters in movinet: {movinet_params}\")\n",
    "print(f\"number of parameters in mobilenet large: {mobilenet_large_params}\")\n",
    "print(f\"number of parameters in mobilenet small: {mobilenet_small_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load videos with torchvision.io: 5600.379940986633\n",
      "Time to load videos with ffmpegio: 6023.769095897675\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataloader import custom_collate\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from ffmpegio.video import read\n",
    "from torchvision import transforms\n",
    "\n",
    "class PicklebotDataset(Dataset):\n",
    "    def __init__(self, annotations_file, video_dir, transform=None,target_transform=None,dtype=torch.float32):\n",
    "        self.video_labels = pd.read_csv(annotations_file,engine='pyarrow',header=None)\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_labels)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_labels.iloc[idx,0])\n",
    "        video = ((read_video(video_path,output_format=\"TCHW\",pts_unit='sec')[0]).to(self.dtype))/255\n",
    "        label = self.video_labels.iloc[idx,1]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return video, label\n",
    "\n",
    "class PicklebotDatasetFFMPEGIO(Dataset):\n",
    "    def __init__(self, annotations_file, video_dir, transform=None,target_transform=None,dtype=torch.float32):\n",
    "        self.video_labels = pd.read_csv(annotations_file,engine='pyarrow',header=None)\n",
    "        self.video_dir = video_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_labels)\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_labels.iloc[idx,0])\n",
    "        video = ((torch.tensor(read(video_path)[1])).to(self.dtype))/255\n",
    "        video = video.permute(0,3,1,2)\n",
    "        label = self.video_labels.iloc[idx,1]\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return video, label\n",
    "    \n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "batch_size=8\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "#annotations paths\n",
    "train_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/train_labels.csv'\n",
    "val_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/val_labels.csv'\n",
    "\n",
    "#video paths\n",
    "train_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/train_all_together'\n",
    "val_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/val_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "#dataset     \n",
    "train_dataset = PicklebotDataset(train_annotations_file,train_video_paths,transform=transform,dtype=dtype)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=8)\n",
    "\n",
    "ffmpegio_dataset = PicklebotDatasetFFMPEGIO(train_annotations_file,train_video_paths,transform=transform,dtype=dtype)\n",
    "ffmpegio_loader = DataLoader(ffmpegio_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=8)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for i, (video, label) in enumerate(train_loader):\n",
    "    pass\n",
    "\n",
    "print(f\"Time to load videos with torchvision.io: {time.time() - start}\") #5625.347644805908 seconds\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, (video, label) in enumerate(ffmpegio_loader):\n",
    "    pass\n",
    "\n",
    "print(f\"Time to load videos with ffmpegio: {time.time() - start}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
