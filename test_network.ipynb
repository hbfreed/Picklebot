{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For testing our network'''\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "from mobilenet import MobileNetLarge2D, MobileNetSmall2D, MobileNetSmall3D,MobileNetLarge3D\n",
    "from movinet import MoViNetA2\n",
    "from helpers import calculate_accuracy, calculate_accuracy_bce\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "def forward_pass(loader,model):\n",
    "    model.eval()\n",
    "    test_losses = [] \n",
    "    test_samples = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    #calculate the loss\n",
    "    for test_features,test_labels in tqdm(loader):\n",
    "        test_features = test_features.to(torch.bfloat16).to(device)\n",
    "        test_labels = (test_labels.long()).to(device) \n",
    "        test_outputs = model(test_features)\n",
    "        # test_labels = test_labels.unsqueeze(1)\n",
    "        test_loss = criterion(test_outputs,test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        test_correct += calculate_accuracy(test_outputs,test_labels)\n",
    "        \n",
    "        test_samples += len(test_labels)\n",
    "        \n",
    "\n",
    "    return test_losses, test_correct, test_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    #evaluate the model\n",
    "    balls_losses, balls_correct, balls_samples = forward_pass(ball_loader,model)\n",
    "    strikes_losses,strikes_correct, strikes_samples = forward_pass(strike_loader,model)\n",
    "    test_losses = balls_losses + strikes_losses\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    print(balls_correct)\n",
    "    print(strikes_correct)\n",
    "    balls_accuracy = balls_correct / balls_samples \n",
    "    strikes_accuracy = strikes_correct / strikes_samples \n",
    "    return avg_test_loss, balls_accuracy, strikes_accuracy \n",
    "\n",
    "def state_dict_converter(state_dict):\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"_orig_mod.\"):\n",
    "            new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    return state_dict\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "batch_size = 4 \n",
    "#annotations paths\n",
    "strike_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/strike_test_labels.csv'\n",
    "ball_annotations_file = '/home/henry/Documents/PythonProjects/picklebotdataset/ball_test_labels.csv'\n",
    "\n",
    "#video paths\n",
    "test_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/test_all_together'\n",
    "\n",
    "#establish our normalization using transforms, \n",
    "#note that we are doing this in our dataloader as opposed to in the training loop like with dali\n",
    "transform = transforms.Normalize(mean,std)\n",
    "\n",
    "\n",
    "ball_dataset = PicklebotDataset(ball_annotations_file,test_video_paths,transform=transform)\n",
    "strike_dataset = PicklebotDataset(strike_annotations_file,test_video_paths,transform=transform)\n",
    "ball_loader = DataLoader(ball_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "strike_loader = DataLoader(strike_dataset, batch_size=batch_size,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "\n",
    "model = MobileNetLarge3D(num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "state_dict_36 = torch.load(f'/home/henry/Documents/PythonProjects/Picklebot/models/MobileNetLarge3D36.pth')\n",
    "state_dict_48 = torch.load(f'/home/henry/Documents/PythonProjects/Picklebot/models/MobileNetLarge3D48.pth')\n",
    "\n",
    "\n",
    "model.load_state_dict(state_dict_36)\n",
    "model.to(torch.bfloat16)\n",
    "avg_test_loss,balls_accuracy,strikes_accuracy = estimate_loss()\n",
    "print(f'Mobilenet Large 36 test loss: {avg_test_loss:.4f}, ball test accuracy: {balls_accuracy * 100:.2f}% strike test accuracy: {strikes_accuracy * 100:.2f}%')\n",
    "model.load_state_dict(state_dict_48)\n",
    "model.to(torch.bfloat16)\n",
    "avg_test_loss,balls_accuracy,strikes_accuracy = estimate_loss()\n",
    "print(f'Mobilenet Large 48 test loss: {avg_test_loss:.4f}, ball test accuracy: {balls_accuracy * 100:.2f}% strike test accuracy: {strikes_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we test the ensemble of 48 and 36. \n",
    "from mobilenet import MobileNetLarge3D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from psutil import cpu_count\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "from dataloader import PicklebotDataset, custom_collate\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn',force=True)\n",
    "\n",
    "def calculate_accuracy(outputs,labels):\n",
    "    predicted_classes = torch.argmax(outputs,dim=1).to(labels.device)\n",
    "    num_correct = torch.sum(predicted_classes == labels).item()\n",
    "    return num_correct\n",
    "\n",
    "def state_dict_converter(state_dict):\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"_orig_mod.\"):\n",
    "            new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    return state_dict\n",
    "\n",
    "std = (0.2104, 0.1986, 0.1829)\n",
    "mean = (0.3939, 0.3817, 0.3314)\n",
    "annotations_file = \"/home/henry/Documents/PythonProjects/picklebotdataset/test_labels.csv\"\n",
    "test_video_paths = '/home/henry/Documents/PythonProjects/picklebotdataset/test_all_together'\n",
    "transform = transforms.Normalize(mean,std)\n",
    "dataset = PicklebotDataset(annotations_file,test_video_paths,transform=transform)\n",
    "loader = DataLoader(dataset,batch_size=4,shuffle=True,collate_fn=custom_collate,num_workers=cpu_count())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "state_dict_36 = torch.load(f'/home/henry/Documents/PythonProjects/old_checkpoints/MobileNetLargeWeighted/MobileNetLarge3D36.pth')\n",
    "state_dict_48 = torch.load(f'/home/henry/Documents/PythonProjects/old_checkpoints/MobileNetLargeWeighted/MobileNetLarge3D48.pth')\n",
    "model_36 = MobileNetLarge3D()\n",
    "model_36.load_state_dict(state_dict_converter(state_dict_36))\n",
    "model_48 = MobileNetLarge3D()\n",
    "model_48.load_state_dict(state_dict_converter(state_dict_48))\n",
    "\n",
    "model_36.to(device)\n",
    "model_48.to(device)\n",
    "model_36.eval()\n",
    "model_48.eval()\n",
    "test_losses = 0\n",
    "test_samples = 0\n",
    "test_correct = 0\n",
    "\n",
    "#forward pass\n",
    "for videos, test_labels in tqdm(loader):\n",
    "    videos = videos.to(device)\n",
    "    test_labels = (test_labels.long()).to(device)\n",
    "    with autocast():\n",
    "        out_36 = model_36(videos)\n",
    "        out_48 = model_48(videos)\n",
    "    avg_out = (F.softmax(out_48,dim=1)+F.softmax(out_36,dim=1))/2\n",
    "    test_loss = criterion(avg_out,test_labels)\n",
    "    test_losses += test_loss.item()\n",
    "    test_correct += calculate_accuracy(avg_out,test_labels)\n",
    "    test_samples += len(test_labels)\n",
    "\n",
    "avg_accuracy = test_correct/test_samples\n",
    "avg_loss = test_losses/test_samples\n",
    "\n",
    "\n",
    "print(f\"As an ensemble, average accuracy: {avg_accuracy}, avg_loss: {avg_loss}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The directory '/home/henry/Documents/PythonProjects/first_layer_filters' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [im]\n\u001b[1;32m     30\u001b[0m ani \u001b[38;5;241m=\u001b[39m FuncAnimation(fig, update_frame, frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m---> 31\u001b[0m writergif \u001b[38;5;241m=\u001b[39m \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/henry/Documents/PythonProjects/first_layer_filters/first_layer_filter_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.gif\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m ani\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/henry/Documents/PythonProjects/first_layer_filters/first_layer_filter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m, writer\u001b[38;5;241m=\u001b[39mwritergif)\n\u001b[1;32m     33\u001b[0m writergif\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/v2.py:324\u001b[0m, in \u001b[0;36mget_writer\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    322\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 324\u001b[0m image_file \u001b[38;5;241m=\u001b[39m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimopen_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_file, LegacyPlugin):\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_file\u001b[38;5;241m.\u001b[39mlegacy_get_writer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/core/imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     request\u001b[38;5;241m.\u001b[39mformat_hint \u001b[38;5;241m=\u001b[39m format_hint\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bytes>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/core/request.py:247\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/imageio/core/request.py:412\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    410\u001b[0m dn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(fn)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(dn):\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe directory \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m dn)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The directory '/home/henry/Documents/PythonProjects/first_layer_filters' does not exist"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's try visualizing the first layer of filters\n",
    "from mobilenet import MobileNetLarge3D\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "from matplotlib.animation import FuncAnimation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def state_dict_converter(state_dict):\n",
    "    for key in list(state_dict.keys()):\n",
    "        if key.startswith(\"_orig_mod.\"):\n",
    "            new_key = key.replace(\"_orig_mod.\", \"\")\n",
    "            state_dict[new_key] = state_dict[key]\n",
    "            del state_dict[key]\n",
    "    return state_dict\n",
    "\n",
    "state_dict_36 = torch.load(f'/home/henry/Documents/PythonProjects/old_checkpoints/MobileNetLargeWeighted/MobileNetLarge3D36.pth')\n",
    "model = MobileNetLarge3D()\n",
    "model.eval()\n",
    "model.load_state_dict(state_dict_converter(state_dict_36))\n",
    "weights = model.block1[0].weight.clone().detach()\n",
    "for i in range(weights.shape[0]):\n",
    "    arr = weights[i].cpu().numpy()\n",
    "    fig,ax = plt.subplots()\n",
    "    im = ax.imshow(arr[0])\n",
    "    def update_frame(num):\n",
    "        im.set_array(arr[num]%arr.shape[-1])\n",
    "        return [im]\n",
    "    ani = FuncAnimation(fig, update_frame, frames=range(arr.shape[-1]))\n",
    "    writergif = imageio.get_writer(f'/home/henry/Documents/PythonProjects/first_layer_filters/first_layer_filter_{i}.gif', mode='I', duration=0.1)\n",
    "    ani.save(f'/home/henry/Documents/PythonProjects/first_layer_filters/first_layer_filter_{i}.gif', writer=writergif)\n",
    "    writergif.close()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in movinet: 4660762\n",
      "number of parameters in mobilenet large: 4191584\n",
      "number of parameters in mobilenet small: 1672816\n"
     ]
    }
   ],
   "source": [
    "'''Calculate the number of parameters in each model, for comparison purposes. \n",
    "   Note that movinet is about 2.8x larger than mobilenet small, and mobilenet large is about 2.5x larger than mobilenet small.'''\n",
    "\n",
    "from movinet import MoViNetA2\n",
    "from mobilenet import MobileNetLarge3D\n",
    "movinet = MoViNetA2()\n",
    "mobilenet_large = MobileNetLarge3D()\n",
    "mobilenet_small = MobileNetSmall3D()\n",
    "\n",
    "movinet_params = sum(p.numel() for p in movinet.parameters())\n",
    "mobilenet_large_params = sum(p.numel() for p in mobilenet_large.parameters())\n",
    "mobilenet_small_params = sum(p.numel() for p in mobilenet_small.parameters())\n",
    "print(f\"number of parameters in movinet: {movinet_params}\")\n",
    "print(f\"number of parameters in mobilenet large: {mobilenet_large_params}\")\n",
    "print(f\"number of parameters in mobilenet small: {mobilenet_small_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
